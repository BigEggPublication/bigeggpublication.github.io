# 堆、平衡树和散列表

目标

* 了解二叉堆数据结构，以及如何去实现它。

* 了解AVL平衡树数据结构，以及如何实现它。

* 了解散列表数据结构，以及关于它的实现方法的基础知识。

## 简介

我们在前面已经介绍了许多基本的数据结构，也通过介绍C++编程语言来让我们能够更好地理解内存分配和释放的底层细节。
现在开始，在这本书的后面的部分，我们会研究一些更高级的数据结构和算法。
我们也会讨论一些关于Python和C++语言的实现的问题。
一般来说，我们会提供Python的实现，然后再让你通过C++来实现它，这样你就可以继续学习和提高你的C++技能了。

到目前为止，我们所提到过的数据结构都是用来让我们存储以及检索信息的容器对象。
列表对象能够让我们按照用户定义的顺序来存储数据。
基于数组的列表可以支持按位置进行的有效访问；
但是，如果我们没做到把列表按照一定顺序进行存储，它并不能提供对特定元素的高效搜索。
插入和删除操作，对于基于数组的列表来说效率也不高。
我们还学习了支持高效插入和删除操作的列表的链式实现，但是链表需要更多的内存，而且不支持按照位置进行高效的访问。
堆栈和队列也是容器对象，但它们不像列表那样通用。
它们只支持按照特定顺序来访问数据，而不能用于随机搜索或者随机访问。
树结构对于存储分层数据或者用于存储信息是很有用的，这是因为我们可以更高效地搜索树结构，比如说像是二叉搜索树这样的情况。

在这一章里，我们将研究一些其他的容器数据结构。
优先队列以及堆，对于存储数据然后高效地按照排序之后的顺序访问数据来说是非常有用的。
平衡二叉树是基本二叉搜索树的扩展，无论插入元素的顺序是什么样的，平衡二叉树都能够保持树的平衡结构。
这也就能够保证我们搜索元素的效率始终是一个$Θ(\lg n)$操作。
散列表（也叫散列表）是这样一种能够提供非常有效地插入、删除和搜索操作的数据结构。
你应该已经非常熟悉散列表了，因为这就是Python的词典。
我们将在这一章里研究堆、优先队列、平衡树和散列表的实现细节。

## 优先队列和堆

在5.3节里，我们学习了先进先出队列。
在某些情况下，我们会希望能够按照元素的优先级来进行处理。
比如说，你会按照优先级来考虑需要处理的任务的顺序。
通常来说，大家常用的对事物的优先级判断是到期时间。
对于两天内就会到期的任务，你可能会给予更高的优先级，即使这个两天内会到期的任务是晚于还有一周到期的任务之后分配给你的。
如果你在分配任务的时候把这些任务放入一个队列里，那么稍后到期的那个任务将会在马上到期的那个任务之后出队。
另一个常见的例子是：
医院里急诊室会根据受伤或者疾病的严重程度来优先考虑他们对患者的治疗顺序。
你的计算机操作系统也会对要执行的程序进行优先级排序，从而可以保证关键的操作系统任务和交互式程序能够更频繁地访问CPU。
处理这些类型情况的数据结构被称为*优先队列*（*priority queue*）。
在优先队列里的每个元素都会被分配到一个优先级的值，这些值会被用来确定下一个应该出队的最高优先级元素。
通过Python语法来编写的优先队列的规范是：

```Python
class PQueue(object):

    def enqueue(self, item, priority):
        '''post: item is inserted with specified priority in the PQ'''

    def first(self):
        '''post: returns but does not remove highest priority item from the PQ'''

    def dequeue(self):
        '''post: removes and returns the highest priority item from the PQ'''

    def size(self):
        '''post: returns the number of items in the PQ'''
```

有许多方法都能够去实现优先队列。
一种是按照优先级顺序来维护元素的有序列表，当入队的时候，会把元素放置在适当的位置。
要提到一点，当我们讨论运行时分析的时候，除非我们明确说明了我们正在分析的是最佳情况或者平均情况，我们通常考虑的是在最坏情况下的运行时间。
对于这个实现方法来说，入队操作需要$Θ(n)$时间。
如果我们使用的是数组的话，我们可以通过二分搜索在$Θ(\lg n)$的时间里找到正确的插入位置，但是我们仍然需要移动数组里的元素来插入这个新元素。
最坏的情况是从数组开头进行的插入，因为这需要把所有元素都进行移动，这些移动会需要$Θ(n)$的时间。
如果我们使用的是链表的话，那么就会需要$Θ(n)$的时间来找到正确的插入位置，之后的插入操作可以在$Θ(1)$时间内完成。
当然，我们还有一种选择是：
入队的时候，把元素直接添加到列表的末尾，之后在出队的时候，搜索最高优先级元素。
在这种情况下，出队会需要$Θ(n)$的时间。

在使用我们已经熟悉的数据结构的时候，优先队列的入队以及出队操作都需要$Θ(n)$时间。
为了改进这一点，我们将学习一种被称为*二叉堆*（*binary heap*）的新数据结构。
术语*堆*（*heap*）可以被用来描述计算机科学里的几种不同的数据结构。
计算机科学家们指的是：一个从中分配和释放动态内存的内存池。
在这一章里，我们将使用术语*堆*来表示我们在这里讨论的二叉堆。
二叉堆是一个每个节点都有附加属性的完全二叉树，这个特性是：
一个节点上的元素不能够小于它的子节点里的元素。
如果要以相反的顺序提取元素的话，只需要反转这个属性就行了，也就是让每个节点上的元素都不能大于节点的子节点里的元素就行了。
我们提到过，一棵*完全树*（*complete tree*）代表着：
对于除了最深的一层之外的每一层，都充满了节点，而在最深的一层，节点是从左向右填充的。
图13.1展示了完全二叉树的例子，这两棵树里有一棵是二叉堆，而另一棵——右边的树——不是堆。
在之后的例子里，我们只会显示用来指示堆里的位置所需要的那个值，这个值也可以对应于优先队列里优先级。
而要创建一个优先队列的话，我们还​​需要存储优先级的值以及需要存储在优先队列里的数据。

>     堆        不是堆

图13.1：两棵完全二叉树，只有左边的树满足堆的属性

为了能够让优先队列可以使用二叉堆，我们​​必须要先找到能够以低于$Θ(n)$的时间内从二叉堆里插入以及删除元素的算法。
基于我们对二叉堆的定义，最高优先级的元素将会处于树的根节点，因此，我们可以很容易地找到它，从而以$Θ(1)$的时间去返回它。
在完成这个操作之后，我们还需要去更新这个堆，从而能够让我们把下一个最高优先级的元素放到树的根节点。
基于堆的属性，下一个最高优先级的元素是最初的根节点的两个子元素之一。
我们可以通过把它的两个子节点里的较高优先级的元素移动到根节点，然后对它下面的节点也重复这个过程，直到到达树的底部的时候。
但这里会出现的一个问题是，最终我们可能会得到一个非完全二叉树，这是因为我们从底行向上移动的元素可能并不是这一层里面最右边的节点。
为了避免这种情况发生，我们可以用一种稍微有点不一样的过程。
我们可以暂时将树的底层里最右边的那个元素移动到根节点去，然后通过把它和优先级较高的子节点进行交换，来把它向下移动到树里去。
我们不断重复这个将元素向下移动到子树里的过程，直到这个元素已经移动到了一个满足堆属性的位置。
而且，因为我们移动的是在最低一行的最右侧位置的元素，因此这颗树将继续保持它是一棵完全二叉树。

图13.2展示了将最后一层的最右边的元素向下移动到树里的过程。
在这个例子里，先从堆里删除了最高优先级的元素`9`，然后把堆里的最后一层的最右边的元素（`4`）临时移动到树的根节点。
之后，我们把`4`向下移动到树里去，直到它所在的位置满足了堆属性。
一开始的时候，我们会检查根节点的两个子节点，可以知道优先级较高的元素是`8`，它也大于`4`，所以我们会交换这两个元素。
然后我们会检查`4`所在的当前位置的两个子节点，并且找到优先级较高的元素是`7`。
因为`7`有更高的优先级，我们再交换`7`和`4`。
我们继续检查`4`节点的新的两个子节点，发现他们两个都是更低的优先级（`2`和`0`），所以我们完成了整个操作。
在实际编程里里，我们并不会去交换元素。
而是，我们会跟踪树里的最后一个元素，然后把他们一个一个的移动到需要的位置就行了。
按照同样的这个例子的话，我们会把`8`移动到根节点，把`7`移动到`8`之前的位置，然后把`4`移动到`7`之前的位置，来满足堆的属性。

图13.2：删除`9`并且重新组织堆

我们也可以用类似的过程把一个元素插入到堆里去。
我们先把新的元素放在树的最底一层的最后一个位置，然后通过和它的父节点进行交换来让它向上移动，直到满足堆的属性位置。
这个流程也能保证在结束的时候仍然是一棵完全二叉树。
图13.3展示了这个过程。
元素`8`首先被添加到堆的末尾，然后与`4`进行交换，再之后与`7`进行交换。
这个时候，由于`8`小于它的父节点（`9`），而且也已经满足了堆的属性，所以操作完成。
和删除元素类似，在编程实践里，我们不会直接交换元素。
我们会直接把`4`移动到新的位置，然后移动`6`，最后直接把`8`放在正确的位置上就行了。

图13.3：插入`8`并且重新组织堆

我们提到过，为了让二叉堆能够成为优先队列的更好的实现，入队和出队操作需要比$Θ(n)$更有效。
对于二叉堆的插入和删除操作，移动的最大元素数是树的高度。
而且由于树是完全二叉树，因此它的高度会是$Θ(\lg n)$。
所以，如果我们用二叉堆来实现队列的话，那么优先队列的入队和出队操作都可以在$Θ(\lg n)$时间内执行。
这满足了我们决定的要好于$Θ(n)$的目标。

在我们最初讨论二叉树的时候，我们曾经说过当一棵树是完全二叉树的时候，是可以用数组来实现的，因此我们可以使用数组或者是列表来实现堆，而不用使用链接节点来实现。
就像我们在上面展示的那样，在堆里使用的基本算法都是在树里上下移动元素。
在堆的方法里有两个方法会需要在树里向下移动元素，因此我们将编写一个执行这个操作的方法，并且让这两个方法去调用它。
下面这段Python代码展示了这个方法，它通常被称为`heapify`或者是`percolate_down`，代码片段里还包含了构造函数以及返回堆里的元素数量的方法。
这些方法通过一个叫做`heap`的数组类型的实例变量来创建一个堆，在这个数组里，根节点会位于列表里的位置1的地方。
实例变量`heap_size`用来表示堆里的元素数量。
在数组的索引为`1`的位置开始树结构，会简化父节点、左子节点和右子节点的计算；
这样会比从位置零开始存储根节点更简单。
根节点在位置`1`，那么位置`i`的元素的左子节点是`2 * i`，右子节点是`2 * i + 1`，父节点——使用整数除法（`5 // 2`是`2`）——是`i / 2`。

```Python
# Heap.py
class Heap(object):

    def __init__(self, items=None):

        '''post: a heap is created with specified items'''

        self.heap = [None]
        if items is None:
            self.heap_size = 0
        else:
            self.heap += items
            self.heap_size = len(items)
            self._build_heap()

    def size(self):

        '''post: returns number of items in the heap'''

        return self.heap_size

    def _heapify(self, position):

        '''pre: items from 0 to position - 1 satisfy the heap property
           post: heap property is satisfied for the entire heap'''

        item = self.heap[position]
        while position * 2 <= self.heap_size:
            child = position * 2
            # if right child, determine maximum of two children
            if (child != self.heap_size and
                self.heap[child+1] > self.heap[child]):
                child += 1
            if self.heap[child] > item:
                self.heap[position] = self.heap[child]
                position = child
            else:
                break
        self.heap[position] = item
```

`delete_max`方法被用来返回树的根节点处的元素，它还会使用`_heapify`方法来更新整个堆，从而像我们前面讨论的那样来维护堆的属性。
在下面这段代码片段里，我们也包含了`insert`方法:

```Python
    def delete_max(self):
        '''=pre: heap property is satisfied
           post: maximum element in heap is removed and returned'''

        if self.heap_size > 0:
            max_item = self.heap[1]
            self.heap[1] = self.heap[self.heap_size]
            self.heap_size -= 1
            self.heap.pop()
            if self.heap_size > 0:
                self._heapify(1)
            return max_item

    def insert(self, item):

        '''pre: heap property is satisfied
           post: item is inserted in proper location in heap'''

        self.heap_size += 1
        # extend the length of the list
        self.heap.append(None)
        position = self.heap_size
        parent = position // 2
        while parent > 0 and self.heap[parent] < item:
            # move item down
            self.heap[position] = self.heap[parent]
            position = parent
            parent = position // 2
        # put new item in correct spot
        self.heap[position] = item
```

在某些情况下，可能会有一个我们想要转换成堆的元素列表。
我们可以通过把元素一个一个插入堆里来实现这个目标。
当然，还有一种更有效的办法是：
用`_heapify`方法让树的元素向下移动的相同技术来操作现有数组。
完全二叉树里的叶子节点也不能有违反堆属性的子节点，而且也不能向下移动，这是因为它没有子节点和它进行交换了。
这也就告诉了我们，由于它后面的任何节点都没有子节点了，我们可以从数组的中间开始。
接下来我们可以用自下而上的方式来更新树，直到整个树的节点最终都满足了堆属性。
这个操作可以从有子节点的最后一个节点（也就是数组里的中间的那个元素）开始，通过为树里的每个非叶子节点调用我们的`_heapify`方法来完成。
下面是这个逻辑的Python代码实现：

```Python
def _build_heap(self):

    '''pre: self.heap has values in 1 to self.heap_size
       post: heap property is satisfied for entire heap'''

    # 1 through self.heap_size
    for i in range(self.heap_size // 2, 0, -1): # stops at 1
        self._heapify(i)
```

我们已经知道了`_heapify`方法在最坏情况下的运行时间是$Θ(\lg n)$。
因此这个`_build_heap`方法的运行时间不会超过$Θ(n * \lg n)$。
我们可以看到`_heapify`方法会先从树里的最后一个有子节点的节点开始，然后逐步更接近根节点。
由于每个节点都会调用它，而所有的`_heapify`调用里执行的比较或者移动操作的总数,是每个树节点的高度之和。
对于具有`n`个节点和高度$h = \lg n$的完全二叉树来说，会有有一个节点的高度是$h$，两个节点的高度是$h - 1$，四个节点的高度是$h - 2$，依此类推，有$2 ^ h - 1$个节点的高度为$1$。
在这里，我们不用去追求数学细节，但可以知道它的总和是$Θ(n)$的。
这也就意味着我们的`_build_heap`方法的复杂度是$Θ(n)$的。

我们已经编写了二叉堆，所以我们可以很简单地得到最大元素了。
类似的，我们也可以简单高效地得到最小元素。
只不过，在这个情况下，堆的属性是：对于每个节点，它的子节点的值总会大于自身节点。
这样一来，我们需要编写的会是`delete_min`方法而不是`delete_max`方法。

### 堆排序

我们也可以通过使用`_heapify`以及`remove`两个方法来在$Θ(n * \lg n)$的时间复杂度下堆元素进行排序。
这个算法被称为*堆排序*（*heapsort*）。
可能你最直观的想法是：修改上面的代码来让他们从堆里删除最小的元素；
然后，你就可以重复调用`delete_min`方法，并且把返回的元素添加到数组或者时列表的末尾就行了。
这样做的缺点是：
它需要一个单独的数组或者列表来放你的数据，也就是说所需要的内存量会增加一倍。
而由于每次删除元素的时候，堆的大小都会减小，因此我们可以用数组末尾的那个位置来存放被删除的元素就行了。
按照这个方案，我们需要先构建堆，从而能够从堆里删除最大的元素。
然后在我们每次删除一个元素的时候，我们都可以在删除元素之前把它放在堆的最后一个位置上。
在删除了除索引为`1`之外的所有元素之后，这个数组就已经变得有序了。
但是在这个过程之后，这个数组也就不再满足堆的属性了，因此我们不能把它用于堆了。
甚至，堆的属性其实被反转了，因为所有的元素现在都是从最小到最大进行的排序。
下面这段Python代码实现了堆排序算法:

```Python
    def heapsort(self):

        '''pre: heap property is satisfied
           post: items are sorted in self.heap[1:self.sorted_size]'''

        sorted_size = self.heap_size
        for i in range(0, sorted_size - 1):
            # Since delete_max calls pop to remove an item, we need
            # to append a dummy value to avoid an illegal index.
            self.heap.append(None)
            item = self.delete_max()
            self.heap[sorted_size - i] = item
```

`heapsort`算法可以在现有的堆上进行操作，或者我们也可以在无序数组上先调用`_build_heap`来构建堆。
由于`_build_heap`方法是$Θ(n)$的，而且每个`delete_max`的调用都不会高于$Θ(\lg n)$，因此`heapsort`的总运行时间是$Θ(n * \lg n)$。
另外需要注意的是，在这次调用之后，我们也就只需要再调用`delete_max`方法$n - 1$次就行了，堆里最后剩下的元素也就是在树的根节点的那一个元素，是最小的元素（数组的索引为`1`的位置）。
在$n - 1$次调用之后，数组就会被排列成有序的了。

### 关于堆和优先队列实现的说明

前面提到了，由于堆是一个完全二叉树，因此把它通过数组来实现是有意义的。
这也为我们提供了在树里上下移动元素所需要的对父节点和子节点的高效访问。
而且，如果根节点是位于数组里的索引为1的位置，而不是索引为0的位置的话，那么访问父节点和子节点的公式会更加简单。
这也就意味着数组的大小必须要大于树里所有元素的数量。
在Python里，你必须显式地把一个值存储在列表的索引为0的位置，通常会把`None`作为占位符存在那里。
对于显式分配数组空间的语言，如果数组满了，那么就需要在`insert`方法里去调整数组大小。
这代表着，你需要有一个实例变量来指示树里的元素数量以及数组的最大尺寸。
调整数组大小的时候，通常会加倍新数组的大小。
所以，不会有超过50％的内存被浪费掉，并且调整大小的操作的成本平均到每次插入操作里的话，也就是$Θ（1）$。

就像我们在这一章开头讨论过的那样，如果你有一个二叉堆，那么就能够很容易地实现优先队列了。
优先队列类通常会使用一个二叉堆的实例变量来实现。
优先队列的`enqueue`和`dequeue`方法会调用二叉堆的`insert`和`delete`方法来操纵二叉堆里的实例变量。
优先级通常是一个整数值，而元素则可以是任何一种数据类型。
在C++里，我们会用模板来实现二叉堆和优先队列类。
在提供运算符重载的语言里，插入二叉堆的数据类型需要支持比较运算符。
在Python里，你可以简单地把元素作为`(priority，item)`形式的元组存储在队列里，而且元组的比较会首先对第一部分，也就是优先级的那部分进行比较。
在C++里，你可以创建一个包含两个实例变量的类：整数的优先级和会插入堆以及优先队列里的数据元素。
这个类你也需要为它编写比较运算符，从而能够忽略掉其他的数据元素，只利用整数的优先级来比较元素。

## 平衡树

早前的时候，我们研究过二叉搜索树，也知道了在最坏的情况下的搜索时间是树的高度。
对于普通的二叉搜索树来说，树里有$n$个元素，那么它的高度也可以是$n$。
考虑一下如果按照排了序之后的顺序插入元素会发生什么？
每个节点都只会有一个右子节点，总高度为$n$。
如果树大致平衡的话，那么树的高度会接近$\lg n$而不是$n$。
这也就意味着。如果树是平衡的，那么插入和搜索操作都能够以$Θ(\lg n)$的时间复杂度来运行。
维护平衡树的一个很直观的方法是在插入元素的时候，根据需要来更新树结构。
为了让这个方案成为一个好的解决方案，必须能够有效地来进行这个平衡操作。

于是，第一个我们需要讨论的问题是我们所说的树应该是*平衡*（*balanced*）的。
一棵完美的平衡的树是一棵完整的树。
但是，除非树里的元素数量刚刚比`2`的幂小`1`的话（`1`，`3`，`7`，`15`，`31`，`63`等等），这很明显是不可能的。
由于一棵完整的树的高度为$\lg n$，因此完整的树将会具有和满树相同的最坏情况搜索时间——$Θ(\lg n)$。
那么接下来的问题在于，在每次插入的时候都要重新排列树结构，从而维持它是一棵完整的树的话，在计算上是非常昂贵的。
你可以想想如果元素按照顺序进行插入的话，应该怎么维护完全二叉树，来让自己对这部分内容加深印象。

我们可以减少对于平衡要求的限制，从而让树结构能够有更多的级别。
我们可以从要求根节点的左右子树的高度相同开始。
就像图13.4展示的那样，这些妥协还是不够。
这个时候树的高度会是$n / 2$，因此搜索的时间还是$Θ(n)$。

图13.4：只要求根节点具有相同高度的子树的情况下，树的高度可以是$n / 2$

这应该会让你意识到我们需要在每个节点上都强制要求平衡，但不能要求精确的平衡，不然的话我们就只会有一个满的树。
一个合理的解决方案是要求每个节点的左右子树的高度最多相差`1`。
这个方法是由G. M. Adelson-Velskii（格奥尔吉·阿杰尔松-韦利斯基）和E. M. Landis在20世纪60年代发明的解决方案，也被称为*AVL树*。
图13.5的例子展示了AVL树和非AVL树。
右侧的树不是AVL树，是因为节点`5`的左子树的高度为`1`，而它的右子树的高度为`3`。
而且根节点的子树的高度也相差不止`1`。

图13.5：左侧的AVL树和右侧的非AVL树

那么，下一个问题是：AVL树的最坏情况下的高是多少？
你的直觉可能会让你认为这个高度不会比有相同节点数的满树的高度的两倍更高了，因为在每个节点上，子树的高度最多只能相差`1`。
这说明，满树里的一半的节点可能是空的，而且，这个时候它仍然维持着它的平衡性。
如果高度最多是最佳情况的两倍（$\lg n$）的话，那么AVL树的高度最多也就是$2 * \lg n$。
这可以让我们知道，在最坏的情况下的搜索时间是$Θ(\lg n)$，而这正是我们想要的。
为了让我们确定这种直觉是正确的，让我们先看看一些例子。
图13.6展示了AVL树的最坏情况（也就是，在特定节点数的最大高度的样子）。
事实证明，最糟糕的情况是接近$1.44 * \lg n$，但这个比$2 * \lg n$还要好的结果并不会影响我们计算搜索功能的最坏情况的运行时间。

> * 1节点；高度为1
> * 2节点；高度为2
> * 4节点；高度为3
> * 7节点；高度为4
> * 12节点；高度为5
> * 20节点；高度为6

图13.6：AVL树在最坏情况下的高度

现在我们已经相信了AVL树将会为我们提供我们想要的$Θ(\lg n)$的搜索时间，接下来，我们需要找到一个有效的算法，来让我们在把元素插入二叉搜索树的时候保持平衡性。
Adelson-Velskii和Landis开发了一些用来重新平衡树的算法，我们将在后面看到它们。

为了破坏平衡性，我们必须要把节点插入到现有的叶节点里去（如果节点已经有一个子节点的话，那么再向它添加第二个子节点并不会破坏树里任何一个节点的平衡性）。
你可能已经发现了，基于我们是在破坏平衡性的节点的左子树，或是右子树里去插入新节点的情况下，会有一些对称性。
要在节点处违反AVL属性的话，这个节点必须要有一个高度至少为`2`的子树，这是因为节点的两个子树的高度必须相差至少`2`以上，而且这个新插入的节点也必须被插入到树的高度至少为`2`的地方，只有这样才能破坏掉AVL树的属性。
而由于每个节点最多有两个子节点，因此我们可以有四种情况。

图13.7展示了两个对称的例子。
在第一个例子里，新插入了值`3`。
我们可以从那个导致违反了AVL属性的节点，知道这一点。
这个时候，节点`8`的左子树的高度为`3`，右子树的高度为`1`。
所以是插入到了节点`8`的左子节点的左子树里。
我们通过将节点`5`移动到根节点，并且让初始的根节点成为节点`5`的右子节点来重新排列整个树。
接下来，我们需要找到可以放置节点`7`的位置。
我们可以知道，包含`8`的原始根节点在旋转之后将不再拥有左子节点，这是因为它在旋转之前的左子节点（`5`）向上移动了一个高度，成了新的根节点。
因此，我们可以把节点`7`作为节点`8`的左子节点。
要注意的是，二叉搜索树的顺序也是通过这些对树的结构进行的改变来维护的。

图13.7：两个镜像AVL单旋转

图13.7里的第二个例子显示了第一种情况的镜像。
在这种情况下，新的值插入到了根节点的右子节点的右子树，从而违反了AVL树的属性。
这个时候，节点`3`的右子树的高度为`3`，左子树的高度为`1`，所以我们可以知道刚刚插入的是节点`8`。
因此，我们可以把节点`6`移动到根节点，然后把`5`移动到`3`的右子节点上。
这是因为，节点`3`在旋转之前的右子节点（`6`）向上移动了一个高度，成了新的根节点，所以节点`3`在旋转之后将不再拥有右子节点。

在我们刚刚讨论的旋转示例里，我们在这两种情况下都替换了根节点。
在多数情况下，根节点都不会是违反树的AVL属性的最深节点，并且通常来说，根节点都不会被改变。
这些旋转将只会发生在违反了AVL属性的最深的节点。
图13.8展示的例子是一个类似于我们前面讨论的第一个单一旋转的例子。
在这种情况下，以节点`8`为根的子树是一个具有右子树的根节点的左子节点。
当我们插入节点`3`的时候，在节点`8`违反了AVL属性，我们会对这个树进行之前那样的旋转操作（图13.8里没有显示这部分内容）。
这里的关键点是要认识到，以节点`8`为根节点的子树在插入`3`之前具有的高度和插入并进行了旋转修复之后的树的高度是相同的。
很明显这肯定是成立的，那么对于根节点`10`来说，如果它在插入之前没有违反AVL树的属性的话，那么在插入以及通过旋转重新构建了左子树的情况下，它还是不会违反AVL树的属性。
插入前后的左子树的高度都不会改变，因此它与右子树的高度始终都只会相差最多`1`。
这也就告诉了我们，无论旋转发生什么地方，都只需要最多一次旋转就能使树返回平衡状态。

图13.8：AVL树的单次旋转不会替换根节点的情况

在到目前为止的例子里，新插入的节点都是位于旋转的根节点的下方三个高度的位置。
如果我们插入数字`3`，`2`和`1`，那么`3`就会是被旋转的根节点，而`2`就会是旋转之后的新的根节点。
在这种情况下，新插入的节点就只会在根目录下的两个高度了。
当然，新插入的节点也可以位于旋转的根节点的下方，有四个或更多的高度差。
图13.9展示了这样的一个例子。

图13.9：AVL树的单次旋转，其中新插入的节点位于旋转的根节点的下方四个高度

在这个例子里，节点`13`是新插入的节点。
除了根节点（`4`）之外的每个节点都还是维持着AVL的属性的，所以旋转会发生在根节点这个地方。
如果你已经开始考虑如何去实现插入和旋转相关的逻辑了的话，你可能会发现很难用迭代来实现一个解决方案，因为一旦我们找到了要插入的树的底部的位置，我们就需要在树上一直向上回溯，直到找到违反了AVL树的属性的节点。
而通常来说，树的实现都不会包含指向父节点的指针，因此我们没有一个简单的方法来执行这个操作。
而当每个递归返回的时候，就实际上是从下往上在回溯到根节点的过程，所以，通过递归来实现解决方案会让这个问题更加简单。
我们将在这一节的后面来讨论实现的细节。

我们已经看过了两种特定的节点违反了AVL属性的情况：
插入节点的左子节点的左子树，或者是插入它的右子节点的右子树。
还有两种情况是：
插入节点左子节点的右子树，或者是插入它的右子节点的左子树。
在这两种情况下，单个旋转没办法把树结构恢复到满足AVL属性的情况。
好在，在这两种情况下，可以通过两次旋转来把树恢复到满足AVL属性的情况。

图13.10展示了一个插入了值`4`的AVL树。
可以看到根节点不再满足AVL属性了，因为这个时候它的左子树的高度是`3`，而右子树的高度是`1`。
这个图向我们展示了插入到左子节点的右子树的情况。
我们之前看到过的单个旋转没办法解决这个问题；然而，如果进行两次旋转的话就可以了。
图13.11展示了第一次向左进行了旋转之后的中间结果，以及第二次向右进行了旋转之后的最终结果。
在两次旋转之后，树结构恢复到了满足AVL属性的情况。
我们会把画出插入右子节点的左子树的情况作为练习留给你。
和单个旋转的情况类似的，插入之后违反AVL属性的节点不必是根节点。
图13.10里的根节点`6`也可以是某棵树的左子树或右子树的一部分，而这棵子树与另一棵有恰当高度的树可以共同组成一个满足AVL属性的树。

图13.10：将值`4`插入到AVL树里去

图13.11：第一次旋转后的结果和第二次旋转后的结果

为了实现AVL树，我们必须要能够跟踪节点的高度，这个高度可以被定义为这个节点的两个子树的高度的最大值加`1`。
因此，我们的`TreeNode`类必须包含另一个额外的实例变量来存储它。
我们还需要一个算法来计算节点的高度。
由于节点的高度是根据它的子树的高度来决定的，因此我们需要在插入的新元素的时候从树的底部开始计算高度，在向上移动树的时候去更新高度。
节点的高度将会是`1`加上它的左子树和右子树的高度的最大值。
我们将在之后的关于树节点的例子里，使用下面这个`TreeNode`类和`get_height`函数。

```Python
# TreeNode.py

class TreeNode(object):
    def __init__(self, data=None, left=None, right=None, height=0):

        '''post: TreeNode with specified data and left/right subtrees is created'''

        self.item = data
        self.left = left
        self.right = right
        self.height = height

def get_height(t):

    '''post: returns height of a subtree at node t, empty tree has height -1'''

    if t is None:
        return -1
    else:
        return t.height
```

这段代码里需要注意的是，`TreeNode`类里有一个名为`height`的实例变量，而`get_height`函数并不是这个类的方法；它是一个独立的函数。
我们可以通过`get_height`没有缩进，而`TreeNode`里的方法右缩减来确定这一点。
`get_height`函数能够让我们简单的得到节点的两个子树的高度。
比如说，我们用它来获得节点的左子树的高度，但是这个节点没有左子树（也就是它的`left`实例变量是`None`）的话，这个函数会返回`-1`。
于是，叶节点的两个子树的高度都是`-1`，再加上`1`的话，叶节点的高度就会是零了。
我们不能把`get_height`函数作为`TreeNode`类里的方法的原因是，我们需要对不存在的节点（`None`）调用它，所以，我们把`get_height`设计成了一个独立的函数。
在我们想要确定可能为空的节点的左子树或者右子树的高度（左子节或右子节点为`None`）的时候，这是非常必要的。

下面的代码片段里包含了AVL树的部分Python实现。
树里有一个用来包含根节点的实例变量`root`。
当`root`的值是`None`的时候表示是空树。
因为我们需要向上回溯树的节点并且调整节点的高度，而且可能还要在新插入的节点的某个祖先的位置执行一次或者是两次旋转，所以`insert`方法的递归实现会比迭代实现简单得多。
通过递归调用来在节点的左子树或者是右子树里插入节点，在函数返回的时候会向上回溯树的节点，这就能够让我们在递归调用之后对节点进行适当的更新。
为了能够让程序员在用我们的树调用`insert`方法的时候不用指定`root`实例变量，我们使用了让`insert`方法去调用辅助方法的技术，这个方法会把`root`实例变量作为参数传递进去，继而执行所有的工作。
代码片段里包含了插入到左子树的调用，对于右子树的类似代码将会留作练习。

```Python
# AVLTree.py
from TreeNode import *

class AVLTree(object):

    def __init__(self):

        '''post: creates empty AVL tree'''

        self.root = None

    def insert(self, value):

        '''post: insert value into proper location in AVL tree'''

        self.root = self._insert_help(self.root, value)

    def _insert_help(self, t, value):

        '''private helper method to insert value into AVL (sub)tree with
        root node t'''

        if t is None:
            t = TreeNode(value)

        elif value < t.item:
            t.left = self._insert_help(t.left, value)
            # left subtree height may be now larger than right subtree
            if get_height(t.left) - get_height(t.right) == 2:
                # determine which subtree the new value was inserted
                if value < t.left.item:
                    # insertion into left subtree of left child
                    t = self._left_single_rotate(t)
                else:
                    # insertion into right subtree of left child
                    t = self._right_left_rotate(t)

        else:
            # exercise for reader

        # update height of tree rooted at t
        t.height = max(get_height(t.left), get_height(t.right)) + 1
        return t
```

`insert`方法会把`root`作为参数传递给`_insert_help`函数。
当树为空的时候，必须要能够把`root`修改为新创建的节点，因此`_insert_help`方法还需要能够修改传递给它的参数。
当树不为空的时候，`_insert_help`方法会以`t.left`或者是`t.right`作为参数进行递归调用。
当我们到达会创建新节点的树的底部的时候，传递给`_insert_help`的参数的值就会是`None`了。
创建新节点的时候，我们也需要更新树里位于它上方的节点的`left`或者是`right`实例变量。
但是，这也是传递给`_insert_help`函数的参数的节点的`left`或`right`实例变量，你觉得这样能够修改掉它们么？
这里的关键点是形参`t`被设置成了一个新对象，但是它并不会改变传递的实际参数（`t.left`或者是`t.right`）。
要在Python里解决这个问题，我们必须要传递参数并且返回参数的新值（也就是为什么我们必须要写成`t.left = self._insert_help(t.left)`，以及`_insert_help`方法需要`return t`语句的原因）。
旋转的方法也会遇到同样的情况，因此也需要以类似的方式调用这些方法。
在C++里，我们可以通过引用传递来完成这个功能。
我们将会在这一节的末尾讨论它。

当每次递归调用的`_insert_help`返回的时候，代码会去检查左右子树的高度。
所以，代码会沿着从插入的节点到根的路径检查每一个节点的子树的高度。
如果有任何的节点的子树的高度相差等于`2`的话，就会执行相应的单旋或者双旋。
下面的代码片段是`_insert_help`方法里使用到的单旋和双旋逻辑。
在你要编写的`_insert_help`部分，这部分逻辑需要镜像旋转。
`_left_single_rotate`方法实现了图13.7里的第一种情况。
这里的方法都是根据插入的位置来命名的。
当插入的是左子节点的左子树的时候，会调用`_left_single_rotate`方法。
所以，虽然它被命名为了`_left_single_rotate`，`_left_single_rotate`方法会进行顺时针旋转，你也可以认为是右旋转。
很明显，只要代码能够调用到正确的更新树结构的方法，方法的名称是无关紧要的。

```Python
    def _right_left_rotate(self, t):

        '''private rotation method for inserting into right subtree of
           left child of t'''

        t.left = self._right_single_rotate(t.left)
        t = self._left_single_rotate(t)
        return t

    def _left_single_rotate(self, t):

        '''private rotation method for inserting into left subtree of
           left child of t'''

        grandparent = t
        parent = t.left

        grandparent.left = parent.right
        parent.right = grandparent
        t = parent

        grandparent.height = max(get_height(grandparent.left),
                                 get_height(grandparent.right)) + 1
        parent.height = max(get_height(parent.left),
                            get_height(parent.right)) + 1
        return t
```

在向树里插入新元素的时候，虽然在`_insert_help`方法的代码里并没有非常明显的表示出来，只需要最多一个单旋或者一次双旋就能把树结构恢复到了满足AVL属性的情况。
这是因为，每个子树在插入并且旋转之后的高度会和插入之前的高度相同。
只有当我们插入了一个不需要旋转的元素时，树的高度才会增加。
因此，当递归调用返回的时候，最多会发生一次旋转进行修复。
旋转代码将会在$Θ(1)$的时间内执行，因为它只会去更新一些引用或指针。
而因为树的高度是$Θ(\lg n)$，所以插入过程将需要最多$Θ(\lg n)$的步骤来得到正确的位置，$Θ(\lg n)$的步骤来重新修复树结构，以及零个、一个或者是两个恒定步数的旋转。
也就是说，插入和维护AVL属性的整个算法的运行时间为$Θ(\lg n)$。
相应的，由于树的高度是$Θ(\lg n)$，要实现`search`方法的话，也会是$Θ(\lg n)$的复杂度。

对于从AVL树里删除一个节点的情况，由于为了维护树的平衡性会非常的复杂，因此我们不会在这本书里介绍相关算法的详细信息。
如果需要删除操作的话，一个简单的可行的解决方案是把那个节点标记为非活动状态，并且在树变得不平衡的时候，定期重新构建一个没有这些元素的新的树。
对于理论计算机科学家来说，这并不是一个理想的解决方案，但在实践中，它可能会比开发和调试一个复杂的算法更好，而且开发和调试这个复杂的算法也不会为你节省大量的计算时间。
一般来说，开发人员时间会相对昂贵，而计算时间则会相对便宜，因此有时候会选择一些更简单，即使效率更低的算法。
这并不意味着我们不应该去学习那些最好的算法和数据结构，因为我们还是会有机会需要用到它们，但也确实意味着在有些情况下，不值得去努力实现更复杂的算法和数据结构。
最好的情况是拥有一个经过精心设计、并且经过测试的、高效的数据结构和算法的实现库，你可以把这个库重用到你的各个应用程序里去。

我们刚才应该注意到了`AVLTree`的`_insert_help`方法需要修改传递给它的参数。
在Python里，当我们想要更改参数的时候，我们需要编写诸如`x = f(x)`这样的代码，并让函数返回与实参`x`所对应的形参。
在C++里，我们可以像在8.12.3节里提到过的那样，通过引用传递来完成这项操作。
对于同一个例子来说，函数的原型将会是`void f(int& x)`，而且调用的时候我们可以直接用`f（x）`而不需要函数返回`x`。

在`AVLTree`类的C++实现里，传递给`_insert_help`和旋转方法的参数应该是指向`TreeNode`的指针。
这可能是你第一次尝试在C++里通过引用来传递指针。
它的语法是：在星号后放上`&`符号，表示我们正在传递指针。
比如说，当我们在树结构里存储`int`变量的时候，`_insert_help`函数的原型是`void _insert_help(BinaryTreeNode *&node, int item)`。
要记住星号和`&`符号的顺序的话，可以回忆一下这句话：指针通过引用传递。
指针（星号）在引用（`&`符号）之前。

## 其他的树结构

还有很多其他的树的数据结构，比如说：
平衡二叉树以及非二叉树的一些其他的实现。
我们不会在这本书里去涵盖、讨论这些数据结构的细节。
这里的某些树结构会被用到数据库的实现里去。
如果你有兴趣了解有关这部分主题的更多信息，可以去搜索有关：
*红黑树*（*red black tree*），*B树*（*B-tree*）以及*伸展树*（*splay tree*）的信息。

## 散列表

有了我们对平衡二叉树的实现，查找和插入元素操作在最坏情况下的运行时间是$Θ(logn)$。
散列表是这样一种数据结构：虽然在最坏的情况下查找元素的时间可能是$Θ(n)$，但是在大多数情况下这个操作的时间可以被提高到$Θ(1)$。
就像在这一章的介绍里提到的，Python内置的字典数据类型的实现就是为*散列表*（*hash table*，也叫做哈希表）。
散列表也被称为字典或者是*关联数组*（*associative arrays*）。
和“关联数组”这个名称的字面意思一样，散列表会像Python的字典一样，把键和值关联起来。
标准的数组数据结构能够让我们根据数组里的位置来查找值，而关联数组能够让我们根据键查找值。
实现散列表的目标是为了能够提供高效的插入、删除以及搜索的方法；
而且，我们希望每种方法的通用情况都是$Θ(1)$的效率。

我们将会通过查看一个简化的例子来开始我们对散列表的实现的解释。
在这个例子里所有可能的键的集合不会很大而且也已经预先知道了，大写字母`A`到`Z`的集合可以符合这个要求。
散列表仍然使用数组来存储数据，也就是说我们可以按照位置来快速查找元素。
我们想要做的是根据键的数据来进行查找，因此我们需要一个函数来把键映射到存储键和值的数组里相应的位置。
在我们的例子里，我们需要能够把每个字母都映射到数组里的相应位置。
这个映射的操作的术语是*散列函数*（*hash function*或者是*hashing function*）。
从例子的条件可以知道，在数组大小为$26$的时候，一个简单的散列函数可以把字母`A`映射到位置$0$上，把字母`Z`映射到位置$25$上。
下面的Python函数就是这种散列函数的实现。

```Python
def hash_letter(c):
    '''pre: c is a one character string of a capital letter A-Z'''
    return ord(c) - ord('A')
```

`hash_letter`函数用了Python的`ord`函数来把字母转换为ASCII码，然后再减去字母`A`的ASCII码，于是就得到了一个介于$0$和$25$之间的值。
因此，我们可以把这个函数和一个大小为$26$的数组或列表一起使用。
也就是说，散列函数会把键映射到存储键值的数组或列表里的相应位置。
如果键没有出现在散列表里的话，我们就必须要用一个特殊值来表明这个位置还没有被使用；
在Python里，如果`None`不是一个会被存在散列表里的有效值的话，我们可以使用它来作为这个特殊值。
我们不需要把键存在数组里去，这是因为键都可以被一一映射到数组或列表里的每个位置。
下面是散列表的Python版本的完整实现（没有用Python内置的字典），这个散列表将只允许用大写字母作为键，而且每种方法的运行时间都是$Θ(1)$。

```Python
# HashLetter.py
class HashLetter(object):

    def __init__(self):
        '''post: initializes simplified hash table'''

        self.table = 26 * [None,]

    def __getitem__(self, key):
        '''post: returns value for specified key'''

        assert 'A' <= key <= 'Z'
        pos = ord(key) - ord('A')
        if self.table[pos] == None:
            raise KeyError(key)
        else:
            return self.table[pos]

    def __setitem__(self, key, value):
        '''post: value for specified key is inserted into hash table'''
        assert 'A' <= key <= 'Z'

        pos = ord(key) - ord('A')
        self.table[pos] = value

    def __delitem__(self, key):
        '''specified key is removed from hash table'''

        assert 'A' <= key <= 'Z'
        pos = ord(key) - ord('A')
        self.table[pos] = None
```

`HashLetter`类包含了一个包含`26`个元素的列表的实例变量。
我们会在每个位置都存上值`None`来表示散列表在这个位置不包含任何的值。
在把键值对添加到散列表里的过程里，我们会先通过使用散列函数来把字母键映射到列表里的相应位置，然后再将值存储在列表里的这个位置。
散列表并不支持为同一个键存储多个值（尝试在同一个键存储的第二个值会覆盖掉第一个值）。
当我们尝试查找一个字母键的时候，我们仍然会使用散列函数来把字母键映射到Python列表里的相应位置。
如果列表里的这个位置的值是`None`的话，那么代表这个字母键并不存在于散列表里，而且这个时候代码会像Python内置的字典那样抛出`KeyError`异常。
如果列表里相应位置的值不是`None`的话，那么这就表示它是字母键相关联的值，因此会返回这个值。

而因为我们已经重载了各种括号运算符，因此我们可以像使用Python字典那样，对键`A`-`Z`来使用`HashLetter`类。
下面这段Python交互式命令的例子展示了我们的`HashLetter`类的用法。

```Python
>>> d = HashLetter()
>>> d['a'] = 4
Traceback (most recent call last):
File "<stdin>", line 1, in ?
File "HashLetter.py", line 19, in __setitem__
assert 'A' <= key <= 'Z'
AssertionError
>>> d['A'] = 4
>>> d['B'] = 5
>>> d['A']
4
>>> d['C']
Traceback (most recent call last):
File "<stdin>", line 1, in ?
File "HashLetter.py", line 13, in __getitem__
raise KeyError, key
KeyError: 'C'
>>> d['A']
4
>>> d['B']
5
>>> del d['B']
>>> d['B']
Traceback (most recent call last):
File "<stdin>", line 1, in ?
File "HashLetter.py", line 13, in __getitem__
raise KeyError, key
KeyError: 'B'
>>> d['A']
4
```

在我们事先已经知道了键可能出现的集合的情况下，我们这个简单的例子里中所使用的策略是可以正常工作的。
然而遗憾的是，通常来说，这个策略并不太好。
而且，当键可能出现的集合很大，但是键都不会被用到的时候，这个策略也不能很好地工作。
比如说，我们的可能键的集合是从$0$到$20$亿的所有整数，那么我们的数组就需要非常的大，从而需要比普通计算机有的内存还要多的存储空间。
如果键可能出现的集合是英语单词怎么办？
我们应该使用什么散列函数以及我们的数组应该有多大呢？
如果两个键散列到了数组里的相同位置，我们应该怎么办？
这些都是我们将会在这一节的其余部分进行讨论的实现问题。

第一个需要处理的问题是：
把键转换为数字，从而可以让我们通过数学函数来把它映射到数组里的相应位置。
散列表其实不应该把键的数据类型限制到只能用数字或者字符串；我们其实是可以用任何数据来作为键，不论这个数据是数字、字符串还是具有各种数据成员的类，都可以作为键并使用它的数据来值映射到相应的位置。
在我们那个简单的例子里，我们使用了`ord`函数来把字母映射到基于它的ASCII码的数字。
在想要使用单词这个情况下，我们可以使用部分或者是全部字符来进行处理。
比如说，我们可以用字符串的前两个字符并且把它们的ASCII码加起来，或者是通过一个函数，把这两个字符都乘以某个常量再把得到的结果加起来。
遗憾的是，两个字符的某些序列会经常出现在英语里，于是就会导致许多单词会被散列到数组里的相同位置。
这个问题被称为*碰撞*（*collision*）。
我们可以通过使用更多字母，以及使用基于位置的倍数来减少这种情况的发生。
但是在某些情况下我们没办法去阻止碰撞问题的发生。
下面这个处理字符串`w`的Python函数可以让我们基于四个字幕来生成一个独特的散列值。

```Python
def hash(w, array_size):
    '''pre: w is an ASCII string
       post: returns a value between 0 and array_size - 1'''

    v = 0
    for i in range(min(len(w), 4)):
        v = 128 * v + ord(w[i])
    return v % array_size
```

在我们应用模数运算之前，由于字符的ASCII码是小于$128$的，因此这个散列函数会为这个由四个字母组成的唯一序列生成一个唯一的数字。
但是很明显，以相同的前四个字母开头的单词（比如说，`friend`和`friendship`）仍然会得到相同的值。
而且，散列值可以是一个相当大的数字，所以为了能够保证散列值在数组的大小之内，我们会把计算出的散列值按照数组的大小进行求模运算。
这样就会让我们得到一个从$0$到比数组尺寸小`1`之间的数字了。
因此，如果我们用了这个模数函数，在数组的尺寸小于$128 ^ 4$的时候，在我们例子里那些四个首字母不同的单词也有可能会被映射到相同的位置上。
我们可以用多余四个的字母，但这样就需要用更长的时间来计算散列函数，而且只要我们还是用了模数运算来保证我们没有映射到超出列表末尾的位置，我们仍然可能会把多个单词映射到列表里的同一个位置。

在我们那个简单的例子里，字母按照顺序被映射到数组（`A`映射到位置$0$，`B`映射到位置$1$，依此类推）里去，但是这个顺序对于映射来说并不重要。
当可能的键的集合大于散列表里的允许的键的数量的时候（比如存储多个英语单词的这种情况），一般来说你不会希望这些单词是按照顺序进行映射的。
这种情况下，最重要的是散列函数应该是一个能够被快速计算，从而能够很快地找到映射到我们所期望找到的键值对的位置。
比如，对于我们的单词散列函数来说，除非数组的尺寸非常的大，不然的话里面的元素肯定不会按照顺序进行映射。

就像前面提到过的，不论我们用的是什么散列函数，最终我们都会把多个不同的键映射到数组里相同的位置上，这个情况被称为碰撞。
而且，如果已经知道了散列函数，那么我们通常来说都可以能够找到会映射到相同位置的不同的键。
碰撞问题有两种常见的解决方案，它们被称为*链式*（*chaining*）和*开放定址法*（*open addressing*）。
通过链式，每个数组的键的相应位置都会是一个用来存储键值对的列表。
在最坏的情况下，我们存储的所有键都在同一个位置。
而如果使用开放定址法，当一个键映射到一个已经有键值对的位置，我们会不断地用另一个函数去计算散列值，直到我们为这个键在数组里找到一个空位置。
如果要用这两个方法里的任何一个的话，我们都必须在数组里同时存储上键和值，只有这样，我们才能确定在数组里是不是找到了这个键。
对于我们的简单例子`HashLetter`来说，因为每个键都只能映射到数组里的一个位置，因此不需要用这些方法来处理碰撞。

最简单的开放定址法被称为*线性探测法*（*linear probing*）。
在这种情况下，我们的新散列函数是$f(key，i) = (hash(key) + i) mod size$，其中$i$的值从$0$到$size -1$，其中$size$是散列表的数组大小。
这个函数的意思是：如果在$i = 0$的时候$hash(key)$找不到键的话，那么我们就从$i = 1$开始，看看下一个计算出的数组里的位置有没有这个键，或者说它是一个空位。
在整个过程里，我们会从末尾到开头一直循环地查看计算出的数组里的下一个位置，直到我们找到整个键、或者是一个空位、或者返回到最早的那个散列位置。
很明显，如果我们找到了这个键，那么我们也就找到了它的键值对。
如果我们在找到键之前到了数组里的空位置，那么我们也可以知道这个键不在散列表里。
而只有在散列表已经满了的时候，我们才会再回到原来的位置。
当然对于任何实际使用的散列表来说，在数组变满之前是应该需要对数组大小进行调整的，从而能够让散列表可以良好的工作。

有一个比线性探测更复杂的开放定址法功能，被称为*二次探测法*（*quadratic probing*）。
它会使用散列函数$f(key，i) = (hash(key) + a * i^2 + b * i) \mod size$，其中$a$和$b$是整数常量，$i$和线性探测里是一样的。
这样，当每次找到一个非空的位置而且和键不匹配的时候，这个函数会跳过多个位置，而不会像线性探测那样去查找连续的位置。
这样做会在一段连续的位置产生较少的键值对聚集，但是对于$hash(key)$函数之后具有相同结果两个不同的键来说，还是会需要去搜索相同的一组位置。

还有一种方法被称为*双重散列法*（*double hashing*）。
它使用两个散列函数$h1$和$h2$。如果第一个散列函数$h1$没有映射到键或者是一个空位置的话，那么我们会去计算$h1(key) + i * h2(key) mod size$，其中$i$的值从$0$到$size -1$。
我们会不断地重复这个运算，直到我们找到键或者某个空位。
对于任何开放定址法来说，最坏的情况会要求检查数列里的每个位置。
为了能够让双重散列更好地工作（也就是，不用去检查整个散列表），函数$h2(key)$计算出的值必须是与数组大小相对的一个素数（也就是说，它们不能共享任何素数因子）。
这个问题可以通过让散列表的大小为$2$的幂次方，而且让函数$h2(key)$输出奇数来轻松完成。
在实际工作中，双重散列法会比线性探测法或者二次探测法更好。

下面的Python代码实例展示了使用链式来处理碰撞的散列表。
构造函数会创建一个指定大小的列表。
而且列表里的每个位置都会被初始化为空列表。
当元素被添加到散列表的时候，它们会被附加到外部列表的散列位置所对应的内部列表里去。
对于`_hash`函数来说，只要它可以把数组的大小整合进计算里去，从而能够在调整了数组大小之后还能继续工作就行了。
这个函数并不一定需要是一个非常好的散列函数。
列表`self.coef`被用来保存乘数从而可以被用来计算字符串里的每个字母。
比如说，对于列表`[11, 2, 5]`和单词`cat`来说，我们会计算`11 * ord（'c'）+ 2 * ord（'a'）+ 5 * ord（'t' ）`，然后把这个结果对数组大小进行取模。
当你的散列表快要满了，因此你调整了列表的大小之后，你也需要为`self.coef`列表添加会被使用的更大的系数。
其他的每个方法都使用了这个散列函数来把键映射到数组里的相应位置。
在这里已经还包含了一个`__str__`方法，从而能够让我们可以查看散列表里的内部数据成员的详细信息。

```Python
# HashTable.py
class HashTable(object):
    def __init__(self, size=11):
        self.array_size = size
        self.table = []
        for i in range(self.array_size):
            self.table.append([])
        self.size = 0
        self.coef = [self.array_size, 2, 3, 7, 5, 13]

    def _hash(self, key):
        pos = 0
        for i in range(min(len(key), 6)):
            pos += self.coef[i] * ord(key[i])
        return pos % self.array_size

    def __setitem__(self, key, value):
        pos = self._hash(key)
        for i, (k, v) in enumerate(self.table[pos]):
            if key == k:
                self.table[pos][i] = (key, value)
                return
        self.table[pos].append((key, value))
        self.size += 1

    def __getitem__(self, key):
        pos = self._hash(key)
        for k, v in self.table[pos]:
            if key == k:
                return v
        raise KeyError(key)

    def __delitem__(self, key):

        pos = self._hash(key)
        for i, (k, v) in enumerate(self.table[pos]):
            if key == k:
                del self.table[pos][i]
                self.size -= 1
                return
        raise KeyError(key)

    def __str__(self):

        s = []
        for line in self.table:
            s.append(' ' + str(line))
        return '[\n' + '\n'.join(s) + '\n]'
```

我们将会通过下面的代码片段来展示我们的散列表。
这个例子里，散列表会使用单词作为键，数字会被用来当作它们映射的值（比如说，键“`quick`”的值为`1`）：

```Python
# test_HashTable.py
from HashTable import HashTable
h = HashTable()
i = 0
for s in 'the quick brown fox jumps over the lazy dog'.split():
    h[s] = i
    i += 1
print h
print h.size
print h['jumps']
del h['jumps']
print h
print h.size
try:
    print h['jumps']
except KeyError:
    print 'key error raised as expected'
else:
    print 'key error should have been raised'
```

下面这个代码片段是对于我们的链式散列表的输出。
请确保你了解了每个键所映射到的位置，以及发生了碰撞的时间。

```Python
[
  []
  []
  []
  [('dog', 8)]
  []
  [('the', 6)]
  [('quick', 1), ('jumps', 4), ('over', 5)]
  []
  [('brown', 2)]
  []
  [('fox', 3), ('lazy', 7)]
]
8
4
[
  []
  []
  []
  [('dog', 8)]
  []
  [('the', 6)]
  [('quick', 1), ('over', 5)]
  []
  [('brown', 2)]
  []
  [('fox', 3), ('lazy', 7)]
]
7
key error raised as expected
```

下面这段Python的代码展示了使用开放定址法的线性探测散列表。
当你尝试插入比数组大小的更多的键值对的时候，这里面的代码会不起作用。
后面的一个练习会要求你扩展这段代码来解决这个问题。
代码里的构造函数会创建一个指定大小的列表，并且把列表里的每个位置都初始化为`None`。
在这里，我们还是使用之前那个散列函数。
其他的像是获取、复制以及删除的方法都会使用这个散列函数来把键映射到数组里的相应位置。
在查找键的时候，我们必须不断地进行计算，直到找到键或者是找到`None`为止。
我们在递增`pos`的时候会使用模数函数，从而能够让它从列表的末尾循环回到位置`0`。

```Python
# HashTable2.py
class HashTable(object):

    def __init__(self, size=11):
        self.array_size = size
        self.table = self.array_size * [None]
        self.size = 0
        self.coef = [self.array_size, 2, 3, 7, 5, 13]

    def _hash(self, key):

        pos = 0
        for i in range(min(len(key), 6)):
            pos += self.coef[i] * ord(key[i])
        return pos % self.array_size

    def __setitem__(self, key, value):
        pos = self._hash(key)
        while True:
            if self.table[pos] is not None:
                if self.table[pos][0] == key:
                    self.table[pos] = (key, value)
                    return
            else:
                self.table[pos] = (key, value)
                self.size += 1
                return
            pos = (pos + 1) % self.array_size

    def __getitem__(self, key):
        pos = self._hash(key)
        start = pos
        while True:
            if self.table[pos] is not None:
                if self.table[pos][0] == key:
                    return self.table[pos][1]
            pos = (pos + 1) % self.array_size
            if pos == start:
                raise KeyError(key)

    def __delitem__(self, key):

        # this method is incorrect, see the Exercises
        pos = self._hash(key)
        start = pos
        while True:
            if self.table[pos] is not None:
                if self.table[pos][0] == key:
                    self.table[pos] = None
                    self.size -= 1
                    return
            pos = (pos + 1) % self.array_size
            if pos == start:
                raise KeyError(key)
```

我们会用和之前那个例子类似的方法来展示我们的开放定址法散列表：

```Python
from HashTable2 import HashTable

h = HashTable()
i = 0
for s in 'the quick brown fox jumps over the lazy dog'.split():
    h[s] = i
    i += 1
print '['
for item in h.table:
    print str(item)
print ']'
print h.size
print h['lazy']
del h['lazy']
print '['
for item in h.table:
    print str(item)
print ']'
print h.size
try:
    print h['lazy']
except KeyError:
    print 'key error raised as expected'
else:
    print 'key error should have been raised'
```

在用开放定址法的线性探测散列表的情况下，这个例子的输出是：

```Python
[
('lazy', 7)
None
None
('dog', 8)
None
('the', 6)
('quick', 1)
('jumps', 4)
('brown', 2)
('over', 5)
('fox', 3)
]
8
7
[
None
None
None
('dog', 8)
None
('the', 6)
('quick', 1)
('jumps', 4)
('brown', 2)
('over', 5)
('fox', 3)
]
7
key error raised as expected
```

由于我们在使用链式来处理碰撞的散列表里也用了相同的散列函数，因此我们可以知道`quick`、`jumps`和`over`也像`fox`和`lazy`那样，被散列到了列表里的相同的位置。
从输出可以看到，开放定址法会把它们存储在列表里后面的第一个开放的位置里，而且对于`lazy`来说，会循环整个列表，从而把它存到了列表的开头。

到目前可以很明确的是，为了能够让散列表的实现可以更好​​地工作，散列函数需要尽量少的产生冲突。
为了防止冲突，我们需要确保数组始终有合理数量的空位置，而且散列函数必须把所有可能的键尽量均匀地分布到数组的位置上（换句话说，我们不希望有许多键被映射到一个很小的数组区间里）。
在没有对键的可能情况预先知晓的情况下，设计一个这样的散列函数并不是一件容易的事。

当散列表里的数组几乎满了的时候，我们就需要创建一个更大的数组来保持它的良好性能。
执行这个操作的步骤是：
创建一个新的更大的数组。
创建一个新的、可以把键映射到这个更大的数组的散列函数，然后把旧数组里的每个键值对都重新映射到这个新的更大的数组里去。
创建一个新的散列函数并不只是把模数值修改到新数组的大小（尽管这是需要进行的一个修改）那么简单。
这是因为，继续使用当前的散列函数的话，散列值可能会总是小于这个新的更大的数组的尺寸。
我们可能需要修改这个散列函数，从而能够生成更大的值。
在我们的例子里，`hash`函数使用了一个单词的四个字母，因此我们可以通过使用更多的字母来产生更大的散列值。
而与之类似的另一种选择是，我们可以使用更大的系数。
由于使用了新的散列函数，在新数组里的键会被映射到不同的位置，有些应该会被映射到原本较小的数组的长度之外的索引位置去。
而且，如果新的散列函数没有把键映射到那些地方的话，我们就没有解决掉减少碰撞的问题。
就像我们在第10章里在调整动态数组大小的时候所讨论的那样，调整大小的操作非常昂贵，调整一个散列表的大小的运行时间会是$Θ(n)$。
当然，如果我们每次都把数组的大小翻倍的话，我们在插入$n$个键值对之前就可以不再需要再进行另一次大小调整了，于是我们可以把调整大小的成本分摊到插入$n$个键值对的成本里去，从而可以在每次插入的时候只增加常量时间的成本。

我们提到过，最糟糕的情况是所有的键都被映射到了相同的数组位置，这样会导致散列表所支持的每个方法（`insert`、`search`以及`delete`）都是$Θ(n)$的时间复杂度。
在这种情况下，不论我们有没有使用链式还是开放定址法都并不重要，因为结果仍然会是$Θ(n)$。
在实际操作中，如果我们能够保持散列表里的元素数量和数组的大小成一定比例（比如说，我们可以让数组的大小总是比散列表里的元素数大$50%$），再加上用一个很好的散列函数的话，链式方法里的每个链里的元素数，或者对于开放定址法里找到空位置之前的碰撞数量都会是一个相对较小的常量。
只要能够保持这一特性，在大多数情况下这些方法的复杂度都是$Θ(1)$。

就像我们说过的，创建一个好的散列函数是让一个散列表更高效的关键。
像我们在这一章里随机选择一个系数的话，通常结果都不会很好。
如果你的编程语言提供了内置的散列表（就像Python提供了字典那样）的话，你就应该使用这个实现。
这是因为，这个语言的开发人员一定会花费大量的时间来开发了一个很好的散列表的实现。
如果你必须要编写自己的散列表的话，我们建议你在尝试实现自己的散列表从而能够在实际应用程序里进行使用之前，应该更详细地研究散列表。

## 章节总结

这一章我们介绍了三种高级数据结构，并且讨论了实现它们的高效算法。
下面是这一章里讨论的各种概念的摘要。

* 二叉堆是一个完整的树，它的属性是：
对于树里的每个节点，它的子节点都不会小于它。
这个属性让元素按照从小到大被删除会变得非常高效。
如果你希望首先删除更大的元素的话，那么可以对这个属性进行反转。

* 优先队列不是按照先进先出的规则，而是能够以优先级的高低为关系来删除元素。
二叉堆通常被用来实现高效的优先队列。

* 二叉堆也可以用$Θ(n * \lg n)$的时间来对列表进行排序，但在实际工作里，通常会使用其他的排序算法。

* 通过把元素插入二叉搜索树的同时更新树结构，我们可以确保树保持了近似的平衡。
通过使用这种近似平衡的树，插入和搜索操作都可以在$Θ(\lg n)$的时间内被执行。
AVL树是平衡树的一种实现。

* 散列表是一种把键和值进行一一映射的数据结构。
它们的插入、删除和查找操作，在最坏的情况会是$Θ(n)$，但通常来说，都是$Θ(1)$的运行时间。
Python内置的字典就是使用散列表来实现的。

* 当散列表里的两个键都被映射到了存储键值对的列表或数组里的相同位置的时候，就发生了碰撞。
碰撞的解决方案是链式和开放式寻址。
在编写散列函数的时候，需要尽可能地减少可能发生的碰撞次数。

## 练习

**判断题**

1. 二叉堆会把元素在数组里始终按照有序的顺序进行存储。

2. 我们的堆的实现里的`_build_heap`方法会把元素按照顺序放进数组里去。

3. AVL树总是一棵完整的树。

4. AVL树可以是完整的树。

5. AVL树的`insert`方法的运行时间将始终是$Θ(\log_2 n)$。

6. AVL树的`insert`方法可能需要多次单旋或者一次双旋才能维持AVL的属性。

7. 把元素插入散列表可能会需要$Θ(1)$的时间。

8. 把元素插入散列表可能会需要$Θ(n)$的时间。

9. 把元素插入散列表可能会需要$Θ(n^2)$的时间。

10. 可以用AVL树来实现散列表。

**选择题**

1. 假设一个二叉堆被设计为可以高效地删除最大元素，那么找到这个最大元素但不删除它的运行时间是什么（类似堆栈的`top`方法）？

    a) $Θ(1)$

    b) $Θ(\log_2 n)$

    c) $Θ(n)$

    d) $Θ(n^2)$

2. 假设一个二叉堆被设计为可以高效地删除最大元素，那么删除最大元素并且维护堆属性的运行时间是多少？

    a) $Θ(1)$

    b) $Θ(\log_2 n)$

    c) $Θ(n)$

    d) $Θ(n^2)$

3. 如果你已经有了一个二叉堆的实现，那么应该怎样去实现优先队列？

    a) 将堆实现里的代码部分复制到优先队列的实现里。

    b) 在优先队列里，创建一个二叉堆的实例。

    c) 让优先队列成为二叉堆类的子类。

    d) 以上都不是

4. 对于通过二叉堆来实现的优先队列，它的`enqueue`方法的运行时间是

    a) $Θ(1)$

    b) $Θ(\log_2 n)$

    c) $Θ(n)$

    d) $Θ(n^2)$

5. 对于通过二叉堆来实现的优先队列，它的`dequeue`方法的运行时间是

    a) $Θ(1)$

    b) $Θ(\log_2 n)$

    c) $Θ(n)$

    d) $Θ(n^2)$

6. 二叉搜索树在最坏情况下的高度是

    a) $Θ(1)$

    b) $Θ(\log_2 n)$

    c) $Θ(n)$

    d) $Θ(n^2)$

7. AVL树在最坏情况下的高度是

    a) $Θ(1)$

    b) $Θ(\log_2 n)$

    c) $Θ(n)$

    d) $Θ(n^2)$

8. 把一个元素插入到散列表的最坏情况的运行时间是

    a) $Θ(1)$

    b) $Θ(\log_2 n)$

    c) $Θ(n)$

    d) $Θ(n^2)$

9. 把一个元素插入到散列表的最好情况的运行时间是

    a) $Θ(1)$

    b) $Θ(\log_2 n)$

    c) $Θ(n)$

    d) $Θ(n^2)$

10. 如果你的应用程序需要不断地把数据元素添加到一个数据结构里去，并且偶尔（和`insert`操作交替进行）会按照排序的顺序输出元素的话，你应该使用下面的哪一种数据结构？

    a) 二叉堆

    b) 优先队列

    c) AVL树

    d) 散列表

**简答题**

1. 给定了下面这个二叉堆的数组存储（我们希望能够高效地提取出最小的元素），绘制出当前的树结构，然后在堆里删除一个元素之后绘制结果里的树结构。

    ```Python
    5, 21, 8, 27, 22, 10, 12, 28
    ```

2. 在按照下面这个顺序插入每一个数字之后绘制那个时候的二叉堆（我们希望能够高效地提取出最大的元素）的树结构（也就是一共绘制八个树）。

    ```Python
    2, 43, 25, 10, 6, 12, 55, 4
    ```

3. 绘制这样一个AVL树：一个元素刚刚插入到右子节点的左子树里，导致它现在违反了AVL树的属性。

4. 对于下面这棵AVL树，哪一个节点是刚刚插入的？

5. 对于问题4里的AVL树，必须调用四种旋转方法（`left_single_rotate`，`right_single_rotate`，`right_left_rotate`和`left_right_rotate`）里的哪一种来维护AVL树的属性？

6. 对于问题4里的AVL树，哪个树节点会被作为参数传递给旋转方法？

7. 对于问题4里的AVL树，绘制旋转之后的树。

8. 对于下面这棵AVL树，哪一个节点是刚刚插入的？

9. 对于问题8里的AVL树，必须调用四种旋转方法（`left_single_rotate`，`right_single_rotate`，`right_left_rotate`和`left_right_rotate`）里的哪一种来维护AVL树的属性？

10. 对于问题8里的AVL树，哪个树节点会被作为参数传递给旋转方法？

11. 对于问题8里的AVL树，绘制旋转之后的树。

12. Python里的什么类是通过散列表来实现？

13. 具有`n`个元素的散列表在最佳情况下的查找效率（以$Θ$表示法）是什么？

14. 具有`n`个元素的散列表在最坏情况下的查找效率（以$Θ$表示法）是什么？

15. 在`HashTable`类的开放定址法版本的`__delitem__`方法里，把数组元素直接设置为`None`会造成什么问题？
我们应该怎么才能解决这个问题呢？

16. 当散列表里的元素数量和列表的容量相等的时候，如果你尝试插入一个键值对到开放定址法的散列表里会发生什么？

17. 使用AVL树实现散列表有什么缺点？
每种方法在最坏情况下运行时间是多少？

18. 请描述如果用散列表来高效实现这样一种算法：
这个算法可以从列表里删除重复的元素（或者是创建一个没有重复元素的新列表），而不会更改列表里当前元素的顺序。
这个算法的运行时间是多少？
为什么？

**编程练习**

1. 用二叉堆在Python里编写优先队列类。

2. 用C++编写二叉堆类。

3. 用二叉堆在C++里编写优先队列类。

4. 用Python完成AVL树类。

5. 用C++编写AVL树类。

6. 修改Python的链式散列表例子，让它在散列表里的元素数量达到了数组大小的$70%$的时候，把数组的大小加倍。

7. 修改Python的开放定址法散列表例子，让它在散列表里的元素数量达到了数组大小的$70%$的时候，把数组的大小加倍。

8. 使用链式法在C++里实现散列表。

9. 使用开放定址法在C++里实现散列表。
