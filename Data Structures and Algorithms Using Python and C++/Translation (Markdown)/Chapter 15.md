# 算法技术

目标

* 了解、实施以及分析快速排序算法的效率

* 回顾分治算法，学习如何分析递归算法的效率

* 了解贪心算法和动态规划技术，以及在什么时候可以使用它们

* 了解霍夫曼压缩算法的实现，以及最长公共子序列的算法

* 介绍NP完全问题

## 引言

为操纵这些数据结构，这本书里的大部分内容都集中在数据结构和算法。
在这一章的学习里，我们不会再去学习新的数据结构，我们会着重学习一些可以运用在综合问题上的算法技术。
你可能已经用过了其中的一些技术，不过在这一章里，我们会把它们进行分类。
在解决新问题的时候，这样做将会帮助你去思考哪类技术更适合应用于你当前所面对的问题。
现在，你可能已经注意到了，编程技巧的提升来源于过往你在解决问题的时候所获得的知识和经验。
这一章将为你的知识工具箱添加一些新的工具。

## 分治算法

*分治*是这本书里常见的一个策略的名称。
我们在学习的归并排序算法是分治算法的一个经典的例子。
顾名思义，分治算法的基本思想是把问题分解为多个较小的子问题。
就像我们在6.5.1小节里看到的归并排序算法那样，分治算法通常会有一个步骤是用来将多个子问题的解决方案进行整合，从而形成针对初始问题的解决方案。
因为会把树结构分为各个子树来进行处理，所以很多处理树的算法都可以被视为分治算法。
对于用二分搜索算法来查找元素来说，因为我们会把列表分为一半，并且在这一半里去查找元素，也可以归于分治算法。
不同的是，二分搜索算法并不会像大多数分治算法那样，去合并多个子问题的解决方案。

许多分治算法会被写成递归函数，从而可以对子问题进行递归调用。
然而，分治算法并不是必须写成递归函数的。
二分搜索算法就可以被写成递归或者被写成迭代。
在第6章里我们曾经提到过，对于简单的迭代算法，迭代通常好于递归，这是因为在相同的渐进运行时间下，递归的开销会让它不如迭代版本的算法更有效。
分析递归函数所需的时间复杂度通常会比分析迭代算法解决方案的时间复杂度要更困难一些。
既然许多分治算法都是通过递归实现的，在讨论其他分治算法之前，我们将先讨论如何去分析递归函数的运行时间的相关技术。

### 分析递归函数

你可能还记得，在分析归并排序算法的时候，通过图形的方式，我们了解了递归调用的步骤以及每个级别里分别完成的工作量。
在分析树的相关算法的时候，我们讨论了对于每个树的节点有多少次遍历，以及在每个节点上所需要多少工作量，从而来确定运行时间。
为每次递归调用时所发生的情况绘制一张图片，是确定算法执行的工作量的常用方法。
我们可以用这种技术来帮助我们分析递归的斐波那契函数的运行时间；
从第6章的图6.2里，我们可以确定，计算第$n$个递归的斐波那契数的工作量几乎是得到第$n-1$个斐波那契数所需要的工作量的两倍。
虽然使用图表可以为我们提供一些更直观的理解，但是，因为它们不是一种正规严谨的数学方法，在使用图表的时候，也很容易出错。
因此，我们才会说，分析递归函数的代码会比分析迭的代代码更加困难。

对于递归函数的一些特殊情况来说，我们可以使用简单的方程或者是算法来去确定代码的运行时间。
使用这种算法公式来确定运行时间的主要限制是，当每次调用函数的时候，代码都需要能够生成相同的固定数量的递归调用，并且每个调用都是对问题的相同的固定的部分进行处理。
为了理解这些限制，我们将讨论*递归关系*的相关内容。
递归关系是一个递归的等式。
你可能在学习数学的时候曾经看到过一种被称为差分方程的递归关系。

递归关系的一个例子是$T(n) = T(n/2) + c$。
这个递归关系说明了要解决大小为$n$的问题的时间是解决大小为$n / 2$的问题加上一些固定常数$c$的时间。
因此，二分搜索算法的运行时间也可以被写成这个递归关系。
要求解一个递归关系的话，就意味着需要找到一个没有递归引用的闭合形式的解。
这可能会很难，但是大多数满足上面提到的要求的递归关系都可以被很容易地解决。
我们已经知道了递归关系$T(n) = T(n / 2) + c$的解是$Θ(\lg n)$，因为这也是二分搜索算法的运行时间。
对应于归并排序算法来说，它的递归关系是$T(n) = 2T(n / 2)+ c * n$，这是因为这个算法会进行两次递归调用，每次递归调用的列表大小是初始列表的一半，然后有一个会运行$n$次的循环来合并结果，其中这个$n$是递归调用所创建的列表的大小。
因此，我们也可以知道这种递归关系的解是$Θ(n * \lg n)$。

有一种通常被称为*主定理*（*master theorem*，也叫支配理论）的算法公式，可以被用来求解绝大多数但不是全部的递归关系，它被描述成$T(n) = a * T(n / b) + f(n)$这样的形式。
这个形式对应着我们对于固定数量的递归调用（$a$乘以这个公式）以及问题的固定部分（$n / b$）的限制。
我们可以使用主定理来得到能够被表示成这种形式的递归关系的递归算法的渐近运行时间。[^1]
主定理有三种情况：

1. 如果存在常数$e > 0$，使得$f(n) = O(n^{\log_b {a−e}})$，那么$T(n) = Θ(n^{\log_b a})$。
2. 如果$f(n) = Θ(n^{\log_b a})$那么$T(n) = Θ(n^{\log_b a} ∗ \log_2 n)$
3. 如果存在常数$e > 0$，使得$f(n) = Ω(n^{\log_b {a+e}})$；并且同时存在常数$c < 1$以及对于任意常数$n_0 > 0$都有充分大的$n > n_0$，使得$a ∗ f(n/b) <= c ∗ f(n)$，那么$T(n) = Θ(f(n))$

> [^1] 对于这个理论的全部细节以及这个理论的证明，可以查看Thomas Cormen、Charles Leiserson、Ronald Rivest以及Clifford Stein编写的《算法导论》（马萨诸塞州，剑桥，麦格劳·希尔图书公司，2001年出版），第二版，76-84页

我们之前定义了大$O$和theta（$Θ$）符号之间的差异，但是你还没有看到过omega（$Ω$）符号。
回想一下，大$O$符号是指“小于或等于”，而$Θ$符号是指“等于”。
你可能已经猜到了，$Ω$符号代表大于或等于。
对于主定理的三种情况，我们需要比较$n^{\log_b a}$和$f(n)$的递归关系：$T(n)= a * T（n / b）+ f(n)$。
如果$f(n) < n^{\log_b a}$，那么解就是$Θ(n^{\log_b a})$。
如果$f(n)= n^{\log_b a}$，那么解是$Θ(n^{\log_b a} * \log_2 n)$。
如果$f(n) > n^{\log_b a}$，那么在满足额外条件的时候，解是$Θ(f(n))$。
这个额外的条件意味着主定理不能解决$T(n)= a * T(n / b) + f(n)$的所有递归关系，但对于大多数来说，它都能被使用。
总之，解是$n^{\log_b a}$和$f(n)$里的更大的那个数（假设当$f(n)$更大的时候也同时满足额外条件）。
但是，如果它们一样大的话，那么就把它们乘以$\log_2 n$就能够得到解了。

我们现在来看几个例子。
归并排序算法具有递归关系：$T(n) = 2T(n / 2) + n$。
首先需要计算$\log_b a$，很明显因为$a$和$b$都是$2$，所以它的结果是$1$。
接下来我们就需要对$n^1$和$n$进行比较了，而它们肯定是相同的，这也就告诉了我们需要使用主定理的第二种情况，因此，结果是$Θ(n * \log_2 n)$。
二分搜索算法的递归关系是：$T(n) = T(n / 2) + 1$。
计算$\log_b a$的结果为$0$。
因此，需要对$n^0$和$1$进行比较，它们也是相同的，所以我们还是会使用主定理的第二种情况，也就是说结果是$Θ(\log_2 n)$。

现在让我们来看一些虽然没有任何实际用处，但很简单的Python函数，从而能够对主定理的另外两种情况进行展示。
第一个函数是：

```Python
# recursive.py
def f1(n):
    if n > 1:
        a = f1(n // 3)
        b = f1(n // 3)
        c = a + b
    else:
        c = 0
    for i in range(n):
        c += i
    return c

print f1(20)
```

函数`f1`有递归关系：$T(n) = 2T(n / 3) + n$。
我们计算$\log_3 2$的结果小于$1$，因此可以知道$n^{\log_b a}$会小于$n^1$。
这是主定理的第三种情况。
我们还需要有些是比较大的正整数常量$n$来使得：$2 * n / 3 <= c * n$。
可以非常简单地通过选择$c = 1$和$n_0 = 2$来满足要求。
因此，结果是$Θ(n)$。

我们的第二个例子是

```Python
# recursive.py
def f2(n):
    if n > 1:
        a = f2(n // 3)
        b = f2(n // 3)
        return a+b
    else:
        return 1

print f2(20)
```

函数`f2`有递归关系：$T(n) = 2T(n / 3) + 1$。
计算$\log_3 2$的结果小于$1$，因此我们可以知道$n^{\log_b a}$会大于$n^0$。
这是主定理的第一种情况。
所以，结果是$Θ(n^{\log_3 2})$；
如果精确到小数点后三位的话，也就是$Θ(n^0.631)$。

遗憾的是，主定理并不能对所有的递归函数都有效；我们只能对于符合$T(n) = aT(n / b)+ f(n)$模式的那些递归函数使用它。
像是递归斐波那契函数这样的递归关系是：$T(n) = T(n - 1) + T(n - 2)$，所以，在这种情况下，我们不能使用主定理。
如果我们对例子里的`f1`函数进行修改，把递归调用里的其中一个改成`f1(n / 2)`，而另一个保持不变，那么就不能用主定理来得出结果了。
在这些情况下，你只能像这一节的前面那样，用其他技术来进行运行时的分析。

### 快速排序

我们已经研究了许多分治算法（二分搜索，归并排序，树的各种算法），我们将会在这一章里再研究一种分治算法。
*快速排序*（*quicksort*）算法是一种用在排序里的分治算法，而且名称也非常的契合，虽然在最坏的情况下，这个算法的运行时间也是$Θ(n^2)$，但是它在通常情况下是最快的通用排序算法。
快速排序算法的基本思想非常简单，但像这样创建一种在所有情况下都是可以工作，并且在大多数情况下都很高效的算法需要一定的技巧以及对特殊情况的特别关注。

归并排序算法的一个缺点是它需要一个额外的临时数组，这个临时数组的大小和你想要排序的数组是一样的。
快速排序算法的优点在于它是一个原地算法（也就是说，它不需要第二个数组来完成工作）。
快速排序的基本思想类似于归并排序：
把列表分成两部分并且递归地对每个部分进行排序。
但是如何去完成这一步的操作的细节在两个算法里是不一样的。
快速排序算法首先会从列表里选择一个被称为*基准*（*pivot*）的元素。
然后算法会把所有小于基准的元素移动到列表的左侧，把所有大于基准的元素移动到列表的右侧。
之后，基准元素会被放在列表的两部分之间；
这个位置就会是整个列表排序之后基准元素应该在的正确位置。
下一步就是递归地对基准元素两侧的两个较小的数组再次进行上面那样的排序。
当递归调用到大小为`0`或`1`的数组的时候，递归结束。
可以很清楚的知道，这个算法不需要合并步骤，因为分割步骤会向左移动较小的元素、向右移动较大的元素，并且把基准元素放到中间。
因此，这些元素最终都会按照顺序被放置起来。

在我们尝试编写代码来实现算法之前，我们先来看一下这个算法是如何应用在数组`6`、`7`、`1`、`3`、`2`、`5`、`4`上的例子。
如果我们选择最后一个元素（`4`）作为基准，并且按照算法会向左移动小元素、向右移动大元素，把基准放在中间的话，一个可能的结果是`1`、`3`、`2`，接着是基准`4`，再之后是`6`、`7`、`5`。
这里可以注意一下，`4`现在的位置是最终排序结束之后的数组的正确位置。
我们现在递归地对左侧数组进行排序。
如果我们还是选择最后一个元素（`2`）作为基准，并且把小元素移动到左侧、把大元素移动到右侧的话，我们现在就有了一个有序的部分：`1`、`2`、`3`。
后面我们继续对这个数组的左侧子数组`1`和右侧子数组`3`进行递归调用，由于这两个数组的长度都是`1`，因此这次调用将会立即返回。
因为也会有一边的长度是`1`而另一边有更多的元素的情况，所以如果我们通过递归调用来实现的话，，代码就会更简洁。
我们现在以递归方式对最初问题里的右侧数组`6`、`7`、`5`进行排序。
如果我们还是选择最后一个元素（`5`）作为基准，我们最终可以得到`5`、`6`、`7`。
在这种情况下，因为没有比基准更小的元素，因此基准元素会被移动到最左边。
还是一样的，这个时候的基准元素会处于最终结果里的正确位置，因为这个算法总会把小于基准的元素放在它的左侧，把大于基准的元素放在它的右侧。
我们接下来会递归地对数组`6`、`7`进行排序，并且选择`7`作为基准。
而这个小数组已经有序了，因此整个数组也就是有序的了。

我们还没有讨论过如何去有效地选择基准，从而让整个算法更高效的具体实现细节，也没有讨论过如何把更小的元素向左移动，和把更大的元素向右侧移动。
现在先看一下我们的快速排序算法的第一个版本，但不是最终版本的实现，整个算法展示了如何向左移动较小的元素以及向右移动较大的元素。

```Python
# qswrong.py
# this has a subtle bug
# it will not work if all the elements are equal
def quicksort(a, left, right):
    if left < right:
        pivot = a[right]
        i = left
        j = right - 1
        while True:
            while a[i] < pivot:
                i += 1
            while pivot < a[j]:
                j = j - 1
            if i < j:
                # swap
                a[i], a[j] = a[j], a[i]
            else:
                break
        # swap
        a[i], a[right] = a[right], a[i]
        quicksort(a, left, i - 1)
        quicksort(a, i + 1, right)

a = range(15, -1, -1)
quicksort(a, 0, len(a) - 1)
print a
```

假设列表的大小大于`1`，那么，这个实现会选择列表里的最后一个元素作为基准元素。
然后算法会从列表的左侧开始向后移动，直到找到一个大于基准的元素。
一旦找到了大于基准的元素，算法就就从数组的右端开始，从最后一个元素——也就是基准元素——的左边向前移动，直到找到一个比基准小的元素。
一旦找到了这个小于基准的元素，算法就会去交换它找到的这两个元素，因此小于基准的元素会向左移动，而更大的元素会向右移动。
之后它继续开始从左侧向后移动的过程，直到它又找到一个大于基准的元素，然后重复从右侧的位置向前移动的过程。
一旦这两个内部`while`循环相遇，外部`while`循环就会停止。
因此，`while True`循环里的代码将会运行$n - 1$次，因为这个循环体会在索引变量`i`和`j`相向移动的时候，检查除了基准元素之外的每一个元素。
当两个索引相交的时候，基准会被被放置在这个位置，这个位置是这个元素在最终有序列表里的正确位置。
然后代码会递归地把列表的基准元素的左侧部分，基准元素的右侧部分进行排序。
可以看到，这个算法在任何时候都不会创建这个列表的额外副本，元素始终都在初始列表里进行交换。

当然，我们现在就有了一个问题：这种算法的效率如何？
对这个算法的分析并不像归并排序那么容易，因为进行递归调用的两个列表的大小将会根据输入，以及选择哪个元素作为基准而变化。
你的开发经验应该能够让你知道我们希望两个列表都有相同的大小。
这样的话，因为我们要进行两次对只有列表一半大小的列表进行递归调用，而且把较小的元素向左移动以及把较大的元素向右移动也需要$Θ(n)$的时间，我们就能够得出递归关系是：$T(n) = 2T(n / 2) + n$。
这个递归关系是许多分治算法的常见递归关系，它的解是$Θ(n * \log_2 n)$。
看起来这和归并排序算法是一样的，但是对于快速排序算法来说，$Θ$符号所隐藏的常数会更小，因为我们不会把元素复制到第二个列表去，以及从第二个列表再把元素复制回来。
这对处理大型列表来说是一个重要优势。
使用归并排序，我们提到过需要一个和我们正在排序的数组大小相同的额外数组。
因此，在实际工作中，如果我们可以把列表分成两个差不多相等的一半大小的话，那么快速排序会比归并排序更快。

遗憾的是，快速排序算法不能保证每次都把列表分成两半。
如果列表已经有序了的话，我们的实现会发生什么？
还是选择列表里的最后一个元素作为基准元素，在这种情况下，每次进行递归调用的时候，基准元素将会是我们正在排序的列表里的最大元素。
我们将会把列表拆分成这样的两个列表：一个没有任何元素，另一个列表因为基准元素不被包含在递归调用，只会是初始列表少一个元素。
如果列表的排序是逆向的，也会发生这样的$0$和$n - 1$个元素的分割。
这些情况相当于是：$T(n) = T(n-1) + n$这样的递归关系。
这个公式不符合主定理可以解决的情况，因此我们必须采用不同的方法来进行分析。
一种解决这个情况的方法是继续展开递归关系：

```
T(n) = n + T(n-1)
     = n + (n-1) + T(n-2)
     = n + (n-1) + T(n-2)
     = n + (n-1) + (n-2) + T(n-3)
     = n + (n-1) + (n-2) + (n-3) + T(n-4)
     ...
     = n + (n-1) + (n-2) + (n-3) + ... + 1
```

就像上面展示的这样，这个递归关系的大小相当于是前`n`个整数的和，也就是我们已经知道的$Θ(n^2)$。
因此，快速排序的最坏情况会比归并排序更糟糕，相当于我们在第3章里讨论的原始的迭代排序算法。

而且，更不幸的是，除了在这两种情况下效率低下之外，这种实现也并不完全正确。
如果列表里的所有元素都相同的话，那么这段代码是没办法正常工作的。
我们将会把了解发生了什么以及如何去解决它作为练习留给你。
这也是一个能够说明正确实现快速排序算法有多困难的例子。

根据我们在前面段落里学到的内容，你已经很清楚基准元素的选择对算法的性能至关重要。
列表已经有序的时候，避免最坏情况发生的一种可能的方法是随机选择基准元素。
这也就意味着没有什么特别的输入会导致最坏情况的发生。
另一个常见的方法是检查三个元素，然后选择这三个元素的中位数作为基准元素；
这样做能够增加基准元素更接近列表的中间元素的可能性。[^2]
现在让我们来看一下使用这个方法来选择基准元素的快速排序算法的实现。
这个算法克服了第一个算法那样的缺点，对所有情况下的输入，这个实现都能够正确处理。

```Python
# quicksort.py

def quicksort(a, left, right):

    '''post: sorts a[left:right+1] (i.e., a[left] through a[right])'''

    if left < right - 1:
        pivot = median3(a, left, right)
        i = left
        j = right - 1

        while True:
            i += 1
            while a[i] < pivot:
                i += 1
            j -= 1
            while a[j] > pivot:
                j -= 1
            if i < j:
                # swap
                a[i], a[j] = a[j], a[i]
            else:
                break

        # swap
        a[i], a[right - 1] = a[right - 1], a[i]
        quicksort(a, left, i - 1)
        quicksort(a, i + 1, right)

    elif left < right:
        if a[left] > a[right]:
            a[left], a[right] = a[right], a[left]

def median3(a, left, right):

    center = (left + right) // 2
    if a[center] < a[left]:
        a[left], a[center] = a[center], a[left]
    if a[right] < a[left]:
        a[left], a[right] = a[right], a[left]
    if a[right] < a[center]:
        a[center], a[right] = a[right], a[center]
    a[center], a[right - 1] = a[right - 1], a[center]
    return a[right - 1]
```

> [^2] Thomas Cormen、Charles Leiserson、Ronald Rivest以及Clifford Stein编写的《算法导论》里的练习题（马萨诸塞州，剑桥，麦格劳·希尔图书公司，2001年出版），第二版，76-84页

如果列表里有至少三个元素的话，那么`quicksort`函数就会使用`median3`函数来选择基准元素。
这个函数会查看第一个，中间以及最后一个元素，然后把最小的元素放在第一个位置，最大的元素放在最后一个位置，然后把基准元素放在倒数第二个位置。
然后，快速排序的其他功能和我们的一开始的那个快速排序算法的第一个版本基本上是一样的。
可以注意到，我们是可以从列表的第二个元素来启动索引`i`的，这是因为我们知道放置在第一个位置元素已经被`median3`函数放置了小于或者是等于`pivot`的元素。
类似地，我们也可以在我们放置基准元素的位置的左侧开始索引`j`，这也是因为我们知道了基准元素和一个大于等于它的元素被放在了最后面。
除了这些细微的改变之外，`while True`循环里的代码和我们的之前的实现是完全一样的。
最后的那个`else`语句会处理我们正在排序的列表如果少于三个元素的情况。

为了进一步提高我们实现的算法的速度，我们可以把最上面的那个`if`语句编写成`if (right - left > 10)`，然后在`else`分支里，用选择排序或者是插入排序算法来处理小列表。
这些迭代算法在处理小列表的时候，会比递归算法更快。
由于递归调用在最后都会对小列表进行排序，因此这个修改可以大幅度提高执行速度。

在实践里，快速排序算法的这种改进版本的实现虽然在最坏情况下的运行时间是$Θ(N^2)$，但是会有$Θ(n * \lg n)$的平均运行时间，并且比归并排序和其他的$Θ(n * \lg n)$复杂度的排序算法要更快。
请注意，只要使用了`median3`函数，算法就会把有序列表或者逆序里列表都分割成相等的一半，因此只要有这种改进，算法就仍然是$Θ(n * \lg n)$的运行时复杂度。
只要算法不会重复地把列表拆分成两个非常不均匀的部分，快速排序都会比归并排序快。
事实上，如果算法把列表每次都分割成了固定的百分比大小，比如说：$1/4$和$3/4$这样的两个列表，那么这个算法仍然有$Θ(n * \lg n)$的运行时间。
证明这一部分内容超出了这本书的范围。
尽管我们通常会分析最坏情况下的运行时间，但研究快速排序算法向我们表明了，有些时候平均情况下的分析更为重要（但也更难做）。

## 贪心算法

和大多数计算机科学的术语一样，*贪心算法*（*greedy algorithm*）策略被命名得非常的契合。
被归类为贪心算法的常见模式是：
在做出选择的时候，算法总是选择目前看起来是最好的选择。
贪婪策略通常被用来解决最优化问题。
最优化问题通常包含这些短语之一：
什么是最好的、什么是最小的、或者什么是最大的。
最优化问题的一个例子是：
如果总共有42美分那么最少有多少个硬币？
在美国，你可以选择的硬币有25美分、10美分、5美分、2美分这几个。
这是一个可以用贪婪策略来解决的问题。
贪婪策略的选择总会选择能够选的最大的硬币。
因此从42美分开始，我们先选一个25美分的硬币，还剩下17美分。
现在我们可以选择的最大的硬币是10美分的硬币，于是还有7美分。
这个时候可以用的最大硬币是5美分的，以及一个2美分的硬币。

使用贪婪策略涉及到了两个主要的步骤。
首先是确定如何把贪婪策略的选择应用于问题。
在我们的硬币例子里，我们决定的贪婪策略是始终用最大的可能的硬币。
第二步是证明贪婪策略的选择是能够得到最优解的方案。
还是在我们的硬币例子里，贪婪策略的选择是成功的，这是因为所有的硬币都是五的倍数。
因而这些硬币的价值可以很容易地保证贪婪算法能够被用于所有可能的总数。

遗憾的是，贪婪策略并不能应用到所有的最优化问题里。
考虑这样一个被广泛研究了的问题，被称为：旅行推销员问题。
给定一组城市之间的直线距离，在总距离最小的情况下，得出访问所有城市的顺序。
一个贪婪策略是从你当前的位置选择最近的城市，然后选择距离它最近的城市，以此类推，直到所有的城市都被访问过为止。
这个策略并不会在所有的情况下都找到最短路径。
我们将在第15.5节里再对这个问题进行讨论。

贪心算法可以被用来实现压缩算法。
压缩的基本思想是减少数据所需的存储量。
压缩有两类：*有损*（*lossy*）压缩和*无损*（*lossless.*）压缩。
顾名思义，当你用有损压缩的时候，会丢失一些数据，而且再也没法准确地重现原始数据；
相应的，无损压缩能够让你在解压缩数据的时候得到完整的原始数据。
不论你有没有意识到压缩数据的存在，你都一定使用过压缩数据。
大部分的音频格式（比如说：MP3和AAC）都会使用压缩来减少存储以及播放音频文件所需的数据量。
类似地，大部分视频也通过压缩格式来进行存储以及传输。
数字电视会用有损压缩格式（通常是MPEG-2压缩格式）进行广播；
你也可能听说过的其他的一些常见的视频压缩格式，比如说：MPEG-4以及H.264（它是MPEG-4的一个特殊形式）。
对于音频和视频应用来说，因为不需要完整的原始数据，有损压缩是可以接受的。
在压缩比和音频、视频的质量之间会需要一些权衡。
只要在压缩版本里包含了足够的数据，当数据解压缩之后，音频和视频对于大多数人来说听起来或看起来依然是“足够好”的。
在美国，不同的网络使用不同的压缩比，而且在某些情况下，比如体育运动的快速移动场景里，质量的损失是非常明显的。

对于其他的应用程序来说，有损压缩是不可接受的。
如果你压缩了源代码或者是一篇研究性论文，那么在解压缩的时候一定要能够得到原始版本。
在这一节里，我们将介绍一种简单的使用贪婪策略的压缩算法，被称为*霍夫曼编码*（*Huffman codes*）；
它是由David Huffman（大卫·霍夫曼）在20世纪50年代发明的。
在这一节里，我们将使用纯ASCII的文本作为例子来讨论这个算法，但是这个算法是可以被应用在任何用比特来表示的数据的。

没有压缩的ASCII文件会使用`8`个比特存储来表示每个字母，相对于的unicode会每个字符使用`16`个比特。
霍夫曼编码的基本思想是：
对于需要压缩的文本里出现的频率更高的字母使用更少的比特，而为不常出现的字母使用更多的比特。
对于在我们的文本里经常出现的字母，比如说`a`，`e`和`s`，我们最终可能只会用两个或者三个比特来表示它。
对于不常出现的字母，比如像`z`或者`q`，会用超过8个比特来表示它。
对于大多数有超过几百个字母的文件来说，使用这种短码和长码混合使用时所需要的总比特数将会小于原始的没有压缩文件。
同时，我们在压缩文件里还需要存储有关每个字母的比特代码的信息，从而能够让我们可以解压缩这个文件。
由于存在存储解码信息的开销，尝试压缩非常小的文件会导致产生一个更大的文件。
当然，如果文件本身就很小，也就不需要压缩它。

霍夫曼编码用来创建压缩文件的技术会生成被称为*前置码*（*prefix code*，或者是*前缀码*）的编码；
或更准确来说，被称为*无首码的代码*（*prefix-free code*，或者是*无前缀码*），你会发现这两个术语可以互换进行使用。
对于被认为是无首码的代码，没有编码可以是另一个编码的前置部分。
编码`10`、`011`、`010`和`110`可以组成一个无首码的代码集。
如果我们把这些编码分配给字母`a`、`b`、`c`、`d`，并且有`0111101001010`这样一个比特序列，那么我们就可以通过对每个比特进行处理直到找到匹配的字母，从而轻松地解码整个序列。
在这个例子里，我们会发现前三个比特对应于字母`b`。
我们继续处理后面的比特序列，因此能够找到对应于字母`d`的三个比特。
接下来的两个比特对应于字母`a`，它后面跟着字母`c`的三个比特，最后是字母`a`的两个比特。
由于没有任何的编码是其他编码的前缀，因此这个过程是对信息进行解码的唯一解，我们可以在比特序列匹配其中一个字母的时候，立即停止处理后面的比特，并且输出相应的字母。

可视化并且仔细理解这个算法的简单方法是使用一个这些前置码生成的树。
比特`0`对应于在树里向左移动，而比特`1`则对应于在树里向右移动。
图15.1展示了这个例子里的前置码所生成的树。
这棵树能够让你从树的根节点开始，在处理每个比特的时候都向下移动。
当你到达带有字母的叶子节点的时候，你就输出这个字母，并且在树的根节点处再次启动整个过程。
通过树来表示前置码可以让你更容易来理解它，因此可以看到所有的字母都始终在叶子节点。
如果一个字母不在叶子节点的话，那么这个编码就不是无首码的代码，并且当我们尝试处理它的时候会出现歧义。
考虑编码`0`、`11`和`110`，它们并不是对应于字母`a`、`b`和`c`的无前缀码。
如果我们尝试对`110`进行解码，那么结果是字母`c`还是双字母序列`ba`？
而如果使用无前缀码的话，我们就不会出现这种造成歧义的问题。

图15.1：前置码所生成的树

很明显，现在的问题是应该如何来组成这棵树，从而有最大的压缩比。
就像我们前面说过的，我们希望使用得更频繁的字母有更短的编码，并且使用得更少的字母有更长的编码。
这就意味着常用的字母需要靠近树的根节点，而不常用的字母需要在树的根节点的下方非常远的地方。
因此，我们的第一步是处理输入的文件，来确定每个字母的频率，并且按照频率对字母进行排序。
为了演示这个算法，我们将会使用一个常见的回文“a man a plan a canal panama”作为例子，因为这个回文只用了一小部分字母，从而能够让我们的树维持在比较小的规模。
下面这个表格展示了这个回文的字母的频率：

| 字母 | 频率 |
| --- | --- |
| c | 1 |
| m | 2 |
| l | 2 |
| p | 2 |
| n | 4 |
| 空格 | 6 |
| a | 10 |

回忆一下我们的两个要求：所有的字母必须在叶子节点，以及我们希望不太频繁出现的字母被存储在树的底部附近。
我们将会为每个字母创建一个树节点，并且从树的底部开始构建整棵树。
在图15.2里以图形方式来展示这些节点，这里的每一个节点都会显示出它所代表的字母以及紧跟的频率。
我们通过对它们的出现频率进行升序排列。

图15.2：霍夫曼编码的启动树

霍夫曼开发的算法通过将当前最小的两棵树合并成一棵树来工作。
在我们的例子里，第一步选择字母`c`和`m`，然后把它们合并成一个有根节点的树，根节点的总频率为`3`（两个单独频率的总和）。
在图15.3里展示了这个结果。

图15.3：第一次合并最小的两棵树

接下来，因为`l`和`p`现在是频率最小的两棵树，所以我们选择字母`l`和`p`，然后合并出一棵新的树，就像图15.4展示的那样。
现在两个最小的频率是`3`和`4`，因此我们把它们合并在一起，结果就像图15.5展示的那样。
我们继续这个过程，并且把频率为`4`和`6`的两棵树合并在一起，结果就是图15.6展示的那样。
下一步，把两棵频率为`7`和`10`的树合并在一起。
最后，只剩下了的两棵树，我们把它们合并起来，就得到了最终结果，就像图15.7展示的那样。

图15.4：第二次合并最小的两棵树

图15.5：第三次合并最小的两棵树

图15.6：第四次合并最小的两棵树

图15.7：霍夫曼编码的最终树

| 字母 | 频率 | 编码 | 总比特 |
| --- | --- | --- | --- |
| c | 1 | 1000 | 4 |
| m | 2 | 1001 | 8 |
| l | 2 | 1010 | 8 |
| p | 4 | 1011 | 16 |
| n | 4 | 110 | 12 |
| 空格 | 6 | 111 | 18 |
| a | 10 | 0 | 10 |
| | | | 76 |

在这些图的后面的表格里展示了字母、它们的初始频率、它们的比特编码、以及使用比特编码来存储这个字母所需要的总比特数（频率乘以比特编码的长度）。
对于我们的例子：字符串“a man a plan a canal panama”来说，总共需要`76`个比特来存储。
前面提到过，我们还必须要存储字母和它所对应的编码，从而能够让我们对压缩后的编码进行解码。
在这个例子里，由于字母串的长度太短了，而存储每个字母的编码也需要一定的开销，因此可能会导致压缩之后的文件变大。

要注意的是，在我们合并树的时候，如果有频率相等的情况的话，那么是可以形成多个树的。
为了说明这一点，我们改变最后两步，让频率为`7`的树和包含字母`a`的频率为`10`的树先合并，而不像之前那样去合并包含字母`n`的树。
这就会导致最终的树会像图15.8这样。
你甚至还可以在合并树的时候随意决定某一棵树是左子树还是右子树，这些变化都会导致编码的略微不同。

图15.8：另一种可能的霍夫曼编码

下面这个表格展示了字母、它们的初始频率、图15.8里的树的比特编码、以及使用比特编码来存储这个字母所需要的总比特数。
可以看到，表示这个字母串所需要的总比特数也是`76`个比特。
你应该可以猜到这一点，因为我们用一棵频率总和为`10`的子树替换掉了另一棵频率总和为`10`的树。
而且，在任何一种情况下，都需要相同数量的比特来表示这个子树。

| 字母 | 频率 | 编码 | 总比特 |
| --- | --- | --- | --- |
| c | 1 | 0000 | 4 |
| m | 2 | 0001 | 8 |
| l | 2 | 0010 | 8 |
| p | 4 | 0011 | 16 |
| n | 4 | 10 | 8 |
| 空格 | 6 | 11 | 12 |
| a | 10 | 01 | 20 |
| | | | 76 |

接下来让我们需要解答的两个问题是：
我们如何实现这个算法，以及它的效率如何。
第一步是读取我们需要压缩的文件，并且计算出频率总和。
在Python里，我们可以把字母作为键映射到它们的频率上。
在C++里，除非你已经有一个散列表，不然的话，你也可以通过一个数组来储存字母的频率，因为当我们读取文件的每个字节的时候，我们会知道最多有`256`个可能的值（如果文件是ASCII编码的，那么只有`128`个可能的值）。
因此，我们可以用一个长度为`256`的数组，并且把每个值都初始化为零，然后每次从文件里读取一个字节的时候，在相应的数组的位置上加`1`。
这个算法的复杂度是$Θ(n)$，其中$n$对应于文件的总字节数。
下一步是对频率进行排序。
由于最多需要对`256`个元素进行排序，因此可以把这一步看作$Θ(1)$。
接下来，我们要为每个频率创建一个树节点，并且按顺序来存储它们。
为了能够让我们很好地实现前面图里所展示的算法，二叉堆或者优先队列是我们需要使用的数据结构。
我们从堆里移除两个最低频率的元素，然后插入这两个被移除的元素合并之后所形成的树。
与上一步一样，这里的工作量也是不变的，因为我们总是向堆里插入以及从堆里删除固定数量的元素。
然而，即使我们不考虑这些数字常量的算法，排序以及在堆里插入和删除$n$个元素一样，都需要$O(n * \lg n)$的时间，其中$n$是文件里的字母数量。

我们现在可以通过树来确定每个字母在树里的位置的比特编码了。
我们可以通过一个修改版本的后序遍历来确定每个字母的编码，这个修改版本的后序遍历会添加一个额外的参数，这个参数被用来传递一个和比特序列所对应的列表。
每次移动到左路径的时候，我们会在列表的最后添加一个`0`；
每次跟踪右路径的时候，我们会在列表的最后添加一个`1`。
每次递归调用返回的时候，我们会删除列表里的最后一个元素。
根据树里的节点的数量，这一步的运行时间是线性的。
而且，这一步也可以被认为是常量时间的操作，因为树里最多会有`512`个节点。
下一步是把这个字母和它的编码存储在一个新文件里，然后按照它们在原始文件里出现的顺序存储每个字母的编码。
可以看出来，对于大型文件来说，运行时间主要是被用在读取和写入字母上，并且这个时间和文件的字节总数是线性相关的。

要解压缩一个文件的话，我们就需要读取这个包含文件里的每一个字母的比特编码的头信息。
然后，你就可以构建一颗树或者是创建一个散列表，来把每个编码都和它所对应的字母进行匹配。
然后当文件开始读取的时候，你会一次读取一个比特，你可以像我们在这一节开头说的那样使用这棵树来对它进行解码，或者如果使用散列表的话，继续读取编码直到获得的代码是散列表里的键为止。
整个运行时间还是和文件的字节总数是线性相关的。

要实际压缩一个文件的话，我们通常会需要进行位操作。
大多数编程语言（但并不是全部）会为执行位操作提供运算符。
在Python和C++里，`<<`和`>>`运算符可以用来移位，二进制运算符`&`和`|`可以用来执行位级别的`and`/`or`操作。
下面与Python解释器的交互式操作展示一个关于这些运算符的例子。

```Python
>>> x = 1
>>> x = (x << 1) | 1
>>> x
3
>>> x = (x << 1) | 0
>>> x
6
```

语句`x << 1`会把所有的位都向左移动一个位置，如果最初的值是`1`的话，那么`x`现在就是`2`了。
后面使用的`|`运算符会按位进行“或”运算，因此`x`现在的值是`3`。
再向左对`3`进行左移，在基数为`2`的情况下这个数字被表示为`11`，向左移动一个位置就会得到`110`。
和`0`进行位的“或”运算并不会改变这个数字，所以`x`现在的值是`6`。
霍夫曼压缩算法里，这种类型的位操作可以用来构建编码序列的比特编码。
每当达到一定数量（比如说,`8`个或者是`32`个）的比特的时候，你都可以把这个值写到文件里去，然后再启动这个过程。

最后一个问题是使用这种贪婪策略的压缩有多好。
答案是它对于前置码的实现来说是最优解，因为我们使用了最短的位数来表示最常用的字母。
但是，如果你实现这个算法然后把它和gzip、bzip2或者zip这样的压缩程序进行比较的话，你可能会发现它们压缩的文件的比例会超你的Huffman压缩程序。
这些程序工作得更好的原因是他们会把多个字母组合在一起，并且为这些字母构建编码。
比如说，双字母序列`sh`、`th`以及`ch`就经常出现在英语单词里。
如果我们对这些多字母的序列也使用短比特编码的话，我们就能够比只对单个字母使用比特编码来压缩实现更高的压缩比。

## 动态规划

*动态规划*是另一种经常被用来解决最优化问题的技术，它有点类似于分治算法。
当我们把问题分解成子问题，并且每个子问题都只出现一次的时候，简单的分治算法非常有效。
但是，这种策略对于第6章里的递归版本的斐波那契算法是非常低效的，这是因为它会让我们对相同的子问题进行多次重复的计算。
动态规划解决这个问题的方案是：
通过把每个子问题的答案都存起来，并在需要的时候重用这个存起来的答案，而不是去重新计算这个子问题。
我们的迭代版本的斐波那契算法就可以被归类为动态规划算法。
作为提醒，下面这个代码片段是第6章里的迭代版本的斐波那契函数，它的运行时间是$Θ(n)$。

```Python
def loopfib(n):
    # pre: n > 0
    # returns the nth Fibonacci number

    curr = 1
    prev = 1
    for i in range(n - 2):
        curr, prev = curr + prev, curr
    return curr
```

这是一个非常简单的动态规划的例子，因为在这里我们只需要把最近的两个子问题的答案存储起来就行了，而且很容易把子问题的答案组合起来从而得到当前问题的答案。
和分治算法一样，动态规划算法的一个关键概念是：
确定如何把子问题的解组合成初始问题的解。
在某些情况下，我们只需要分析一个或者两个子问题就能解决初始问题了。
在其他情况下，我们需要多分析一些子问题。
在需要分析多个子问题的情况下，动态规划算法会非常的常见，因为我们经常在解决初始问题的时候需要多次分析相同的子问题。
这个概念通常被称为*重叠子问题*（*overlapping subproblems*）。

有没有重叠子问题并不是决定应不应该用动态规划技术的唯一标准。
最优化问题必须满足的另一个重要标准是：初始问题的解决方案必须包含子问题的最优解。
在设计动态规划算法的时候，确定如何将子问题的最优解组合起来得到初始问题的最优解通常是最困难的任务。

虽然分治算法通常是通过递归来实现的，但是动态规划算法通常会从基本案例问题开始，然后通过它来确定解决更大问题的方法，直到找到初始问题的解决方案。
因此，动态规划算法通常不会使用递归来实现。
恰恰相反，典型的动态规划算法会使用迭代的解决方案，这个解决方案会把子问题的解存在表里，然后直接在表里填写出更大问题的解，直到计算出初始问题的答案。
我们现在将会仔细研究一个可以用动态规划来解决的问题，以及为另一个可以通过动态规划来解决的问题提供基本的算法。

### 最长公共子序列

最长公共子序列问题是一个很容易理解的问题，因此它可以作为动态规划的非常好的第一个例子。
对于这个问题，我们把*序列*（sequence）定义为有限的并且有序的元素列表。
Python的元组、列表或者字符串都满足这一条件，可以被当作序列。
*子序列*（*subsequence*）是初始序列的有序子集；
换句话说，你可以从初始序列里删除一些（也可以不删）元素，但不能更改序列的顺序。
最长公共子序列问题就是：在给定的两个序列里，可以作为两个序列的子序列的最长的序列是什么。
比如说，考虑两个单词`abracadabra`和`batter`作为两个字母序列；
这两个序列的最长公共子序列就是`bar`。
在这个问题里，是可以有多个具有相同最大长度的公共序列存在的，但在这个例子里，`bar`是长度为`3`的唯一一个公共子序列。

暴力算法来解决这个问题可以通过：
对一个序列的所有子序列，查看它是不是另一个序列的子序列，并且一直执行下去直到找到最长的子序列。
对于长度为$n$的序列来说，它的子序列的数量为是$2^n$。
要得到这个结果，可以通过把序列里的每个元素都看成可以选择或者不选择的元素，从而得到一个更容易分析的图像化问题。
这就能够让我们把整个序列当作一个有$n$个二进制数来组成的序列，然后计算它的子序列的数量。
对于$n$个比特来说，可以被用来表示的数据的数量是$2^n$个，因此这也就是说这个得到所有的子序列的过程是$Θ(2^n)$的算法。
之后我们需要再去找最长的子序列，它也是另一个序列的子序列。

对于你来说可能并不能够很直观地知道动态规划可以被用来解决整个问题。
这也就是我们为什么要研究各种算法技术的例子的原因。
学习解决各种不同问题的技术的经验可以帮你了解到对于新问题应该用什么技术来解决。
为了对整个问题使用动态规划，我们需要确定如何根据较小问题的解来找到较大问题的最佳解。
这一步需要你的直觉和经验来帮助你。

我们的第一步是检查一个小问题，因此我们会从长度为`1`的序列开始（也就是问题的基本情况）。
如果两个序列都包含着相同的元素，那么公共子序列的长度为`1`，不然的话就是`0`。
接下来让我们看看如果对两个对我们已经知道最长公共子序列的序列，都分别添加一个字母会发生什么。
如果我们有两个序列并且为每个序列添加一个相同的字符，那么两个更长的序列的最长公共子序列的长度会比两个初始序列的最长公共子序列长`1`。
比如说，对于序列`abcd`和`cabe`来说，这两个序列的最长共同子序列是`ab`。
如果我们把字母`f`添加到两个序列的末尾，那么最长公共子序列就是`abf`。

既然我们已经有了一些初步想法，那让我们绘制一个表格，看看能不能帮助我们确定整个问题的完整步骤。
由于我们有了两个序列，我们将使用第一个例子里的两个序列制作一个二维表。
对于表里的$[i，j]$元素会被用来指代：
一个序列从字母`1`到$i$的子序列以及另一个序列从字母`1`到$j$的子序列的最长公共子序列的长度。
在下面这个表里，我们填上了第一行和第一列。

|   | a | b | r | a | c | a | d | a | b | r | a |
|---|---|---|---|---|---|---|---|---|---|---|---|
| b | 0 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 |
| a | 1 |   |   |   |   |   |   |   |   |   |   |
| t | 1 |   |   |   |   |   |   |   |   |   |   |
| t | 1 |   |   |   |   |   |   |   |   |   |   |
| e | 1 |   |   |   |   |   |   |   |   |   |   |
| r | 1 |   |   |   |   |   |   |   |   |   |   |

左上角的`0`对应着`abracadbra`里的字母`a`和`batter`里的字母`b`之间最长公共子序列的长度。
它右边的下一个位置的元素一个对应着`abracadbra`里的子序列`ab`和`batter`里的字母`b`之间最长公共子序列的长度。
在第一行里有了`1`之后，第一行里的其它元素基于表的元素的定义来说，都必须是`1`。
第一列也是以完全相同的方式进行填写。
一旦我们计算并存储了表里的第一行和第一列之后，就可以开始按照顺序来计算行和列了。
我们在下一面这个更新了的表里填写了第二行里的数字。

|   | a | b | r | a | c | a | d | a | b | r | a |
|---|---|---|---|---|---|---|---|---|---|---|---|
| b | 0 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 |
| a | 1 | 1 | 1 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 |
| t | 1 |   |   |   |   |   |   |   |   |   |   |
| t | 1 |   |   |   |   |   |   |   |   |   |   |
| e | 1 |   |   |   |   |   |   |   |   |   |   |
| r | 1 |   |   |   |   |   |   |   |   |   |   |

如果我们来考虑一下每个表里的元素的含义的话，我们可以根据它上面、左边、以及对角线左上方的元素来确定如何去计算每个元素。
就像前面提到过的那样，在算法开发的时候需要一些直觉，于是：
如果第一个序列里在位置$i$处的元素和另一个序列里在位置$j$处的元素相同的话，我们可以把第一个序列里到位置$i-1$和另一个序列到位置`j`的最长公共子序列的长度加`1`。
支持这个想法的例子是第二行数字变成`2`的那个地方。
这个位置对应于`abra`和`ba`的最长公共子序列。
因为我们已经知道了`abr`和`b`的最长公共子序列的长度是`1`。
那么，当我们把字母`a`添加到这两个序列里的时候，我们就可以把公共子序列的长度加上`1`，因此得到`2`。
对于我们的表格来说，这相当于在元素一样的时候，对左上角的数字加`1`。

我们还需要确定，如果字母一样的话需要做什么。
一个例子是：把字母`c`加进第一个序列，从而得到`abrac`。
然后我们想知道`abrac`和`ba`的公共子序列的长度。
由于字母`c`和`ba`的最后一个字母不一样，因此我们不能增加最长公共子序列的长度。
而且，`abrac`和`ba`的最长公共子序列必然是`abrac`和`b`的最长公共子序列或者`abra`和`ba`的最长公共子序列里面更长的那个。
对于我们的表格来说，这相当于在元素不一样的时候，取左边或者上边的最大值。
这个例子的最终结果会像下面这个表这样。

|   | a | b | r | a | c | a | d | a | b | r | a |
|---| :-: |---|---|---|---|---|---|---|---|---|---|
| b | 0  | 1D | 1L | 1L | 1L | 1L | 1L | 1L | 1L | 1L | 1L |
| a | 1D | 1L | 1L | 2D | 2L | 2L | 2L | 2L | 2L | 2L | 2L |
| t | 1U | 1L | 1L | 2U | 2L | 2L | 2L | 2L | 2L | 2L | 2L |
| t | 1U | 1L | 1L | 2U | 2L | 2L | 2L | 2L | 2L | 2L | 2L |
| e | 1U | 1L | 1L | 2U | 2L | 2L | 2L | 2L | 2L | 2L | 2L |
| r | 1U | 1L | 2D | 2L | 2L | 2L | 2L | 2L | 2L | 3D | 3L |

在有了这个表之后，我们可以去确定最长公共子序列是什么。
要实现这一点的关键是：
当我们在通过对角线加`1`的时候，我们在公共子序列里添加这个字母。
通过追踪我们用来计算长度的元素，我们就能够确定公共子序列的结果。
我们还可以通过在长度里加上箭头来表示出这个值是从哪里计算出的。
对于我们的表格来说，我们使用字母`D`来表示从对角线元素来的，字母`L`表示左箭头，字母`U`表示向上的箭头。
接下来，我们从表格的右下角开始，因此我们有数字3和向左的箭头，代表着有最长的长度为3，并且需要向左移动。
然后我们能够得到对角线箭头，所以我们的共同子序列里的最后一个字母是`r`。
我们继续按照箭头前进，同时在遇到对角线箭头的时候，就在我们的共同子序列的开头插入这个字母。
当箭头指向了表格之外的时候，我们就完成了整个流程，而且找到了最长公共子序列。

对于我们的例子来说，如果我们继续刚才的流程，当我们到达了对应于`abra`和`ba`在表里的位置的时候，我们会插入字母`a`到共同子序列里。
而后，当我们到达了对应于`ab`和`b`在表里的位置的时候，我们会插入字母`b`到共同子序列里。
那个对角线箭头会让我们离开这个表格，从而表明我们的最长共同子序列是`bar`。
在我们的例子里，当左侧和上方的元素相等的时候，我们会选择左侧的值。
如果你的实现是选择上面的值的话，可能会得到有相同最大长度，但是不一样的公共子序列。

这个算法的运行时分析非常简单。
因为我们必须要填写整个表格。
而计算每个元素只需要恒定的时间。
因此，对于长度为$m$和$n$的序列来说，运行时间为$Θ(m * n)$。
对于我们的暴力算法的运行时间的话，因为我们需要创建较短序列的所有子序列，然后检查它们是不是另一个序列的子序列，因此需要至少$Θ(2^n)$的时间，其中$n$是较短序列的长度。
很明显，动态规划算法的效率更高。

我们的算法的一个缺点是它需要$Θ(m * n)$的空间来计算出结果。
最长公共子序列问题的常见应用是DNA匹配，而DNA是一长串的字母。
好在计算机科学的教授Dan Hirschberg开发了一种只需要线性空间的算法，这使得整个新算法对于长序列来说更为有效。

### 记忆化

我们前面曾经提到过，按照固定顺序来存储和计算表格里的元素的迭代方法是被用来实现动态规划算法的常用技术；
但是，我们也可以使用递归来实现它们。
术语*记忆化*（*memoization*）是指：
使用分治算法的递归公式，但是在计算出结果的时候存储结果，这样我们就不需要多次计算相同的问题了。
在进行递归调用来计算元素之前，代码首先会去检查有没有已经计算并存储了的结果。
如果有的话，就不再需要进行递归调用来计算整个问题，我们可以直接使用整个存起来的值就行了。
这为我们提供了和迭代的解决方案相同的运行时间，而且也保留了递归形式的分治算法。
把计算出的值存储在散列表里的一种不错的方法。
这很容易在Python里通过字典来实现。
数组或者列表也可以被用来存储计算得到的值。
斐波那契函数的记忆化实现如下：

```Python
fibm.py
def fibm(n, d=None):
    if n < 2:
        return n
    if d is None:
        d = {0: 1, 1: 1}
    if n - 1 not in d:
        d[n - 1] = fibm(n - 1, d)
    if n - 2 not in d:
        d[n - 2] = fibm(n - 2, d)
    return d[n - 1] + d[n - 2]
```

如果执行整个函数，你会发现它的实际运行时间和迭代版本的斐波那契函数是一样的，而之前纯粹的递归版本的斐波那契函数对于大于`25`的输入参数来说，会要慢得多。
通常来说，通过迭代来实现算法将会比记忆化的递归实现稍快一些，这是因为在递归算法里需要进行更多的函数调用相关的开销。

### 矩阵链乘法

确定把若干个矩阵相乘的最有效的方法也是可以通过动态规划来解决的另一个问题。
在讨论动态规划如何来解决整个问题之前，我们将会首先提供一些关于矩阵的简单背景。
*矩阵*（*matrix*）是一个二维的数字数组。
为了能够让两个矩阵可以相乘，第一个矩阵里的列数必须和第二个矩阵里的行数一样。
比如说，$6$乘$8$的矩阵可乘以$8$乘$4$的矩阵，但是$8$乘$4$的矩阵不能乘以$6$乘$8$的矩阵。
相乘之后得到的矩阵的大小是第一个矩阵里的行数乘以第二个矩阵里的列数。
比如，$6$乘$8$的矩阵乘以$8$乘$4$的矩阵的结果是一个$6$乘$4$的矩阵。
执行整个计算所需要的工作量是第一个矩阵里的行数、第一个矩阵里的列数（也就是第二个矩阵里的行数）以及第二个矩阵里的列数的乘积。
因此，对于$6$乘$8$的矩阵乘以$8$乘$4$的矩阵来说，一共需要$6 * 8 * 4 = 192$个步骤。
如果你对两个矩阵相乘是怎么计算还不熟悉的话，可以简单快速地去网上搜索一下或者问问你的老师。

即使矩阵的大小允许两个操作数的顺序相反，矩阵的乘法也是不满足交换律的。
但是，矩阵的乘法是满足结合律的。
想想怎么去计算把三个大小分别是$2$乘$10$，$10$乘$4$，以及$4$乘$3$的矩阵$A$、$B$和$C$乘起来的工作量。
如果我们计算$(AB)C$的话，那么首先需要把$2$乘$10$和$10$乘$4$的两个矩阵相乘，这样我们就通过`80`个计算步骤得到了一个$2$乘$4$的矩阵。
之后把整个$2$乘$4$的矩阵乘以$4$乘$3$的矩阵需要`24`步，因此总共需要`104`步来计算出最终的$2$乘$3$的矩阵。
如果我们通过$A(BC)$这样首先把$10$乘$4$的矩阵和$4$乘$3$矩阵相乘来计算结果的话，我们会需要`120`步来得到一个$10$乘$3$的矩阵。
然后我们再把$2$乘$10$的矩阵乘以$10$乘$3$的矩阵从而得到相同的$2$乘$3$的矩阵的结果，而这一步的计算需要`60`个步骤，因此总共需要`180`步。
这就意味着我们会希望把矩阵括起来作为$(AB)C$而不是$A(BC)$来通过较少的计算得出结果。

对于三个矩阵来说，就像上面那样，通过结合律把计算括起来的话，也只有两种不同的选择。
但是如果我们有四个矩阵的话$ABCD$，我们的选择就有：$A(BC)D)$、$A(B(CD)$、$(AB)(CD)$、$(AB)C)D$以及$(A(BC)D$。
那么我们应该如何通过动态规划来解决整个问题呢。
要知道的关键点是：如果把计算括起来的最佳方案是先把$ABC$相乘，然后再把它的结果乘以矩阵$D$的话，那么我们就需要找到一个最佳的方案来乘以$ABC$，也就是$(AB)C$或者$A(BC)$里的一个。
这也就是为什么动态规划可以被用来解决整个问题的原因：初始问题的最优解包含着子问题的最优解。
如果我们有五个矩阵的话：$ABCDE$，将它们相乘的最佳方案可能是$(ABC)(DE)$。
在这种情况下，我们也需要找到把$ABC$乘起来的最佳方案。

如果我们有大量的矩阵需要乘起来，那么不同的括号分配可能会导致所需的计算量大不相同。
因此这个确定最佳乘法顺序的问题通常被称为*矩阵链乘法*（*matrix chain multiplication*）问题。
在一开始的例子里，我们看到了在三个矩阵的情况下，总计算步骤的数量也会有很大的差别。
在有更多的矩阵时，这个差异会更加的显著，因此如果你需要把大量的矩阵相乘的话，那么先去确定如何应用结合律是非常值得的。

这个问题的动态规划解决方案会首先计算每三个连续矩阵相乘的最佳方案。
然后会去确定每四个连续矩阵相乘的最佳方案，依此类推。
如果我们有$n$个被标记为$A_0$、$A_1$、...、$A_{n-1}$的矩阵需要相乘的话，这就需要一个$n$乘$n$的表格。
表格里在位置$[i][j]$处的元素被用来表示把矩阵$A_i$乘以$A_j$的最佳步数。
而且，因为表格是对称的（也就是，$[i][j]$里的元素将会和$[j][i]$相同），我们只需要计算表格的一半就够了。
不像最长公共子序列问题那样，在计算表格里的每个元素的时候都需要相同的工作量，计算这个表格的工作量基于不同的$i$和$j$，会有不同的工作量的。
我们将把这个算法的其余细节留作练习。

一些动态规划算法需要一个一维表，而其他的，就像我们在这里的例子里看到的那样，需要二维表。
还有些问题可能需要更高维度的表。
某些算法会需要固定数量的步骤来计算表里的每个元素，而其他的算法则需要不同的计算量来计算表里的元素。
作为一个算法设计者，你的工作是判断动态规划能不能被用来解决问题，如果可以的话，那么要计算出最终结果所需要的最少工作量是多少。

## NP完全问题

由于这是一本入门书，我们将仅仅介绍一些关于*NP完全*（*NP-Complete*，或*NP完备*）问题的基本细节。
“NP”代表的是“非确定性多项式集合”的时间。
NP问题是指你能不能在多项式时间内验证一个问题的解；
我们现在就来讨论这是什么意思。
P集合的问题对应于可以在多项式时间内解决的所有问题。
因此，P是NP的子集。
一个仍在讨论的知名的大难题是：P等于NP，还是P是NP的一个子集。
NP完全问题是一类没有多项式时间解法的问题。
有趣的是，如果某个NP完全问题可以在多项式时间内得到解，那么所有的NP完全问题都可以在多项式时间内得到解。

这一章前面提到的旅行推销员问题就是一个NP完全问题。
解决旅行推销员问题的唯一已知算法是测量所有可能的路径，然后再选择最短的路径。
但是，如果你有$n$个城市的话，那么就会有$2^n$个可能的路径，因此这个算法的效率极低。
如果有人告诉你某条路径的长度是`1,000`，那么我们可以在多项式时间（实际上是线性时间）内验证这个路径的长度是`1,000`。
因此，这个旅行推销员问题也属于NP问题。
而且，由于没有已知的多项式时间算法来解决它，因此它是NP完全的。

计算机科学专业的研究生通常会学习如何去证明一个问题是NP完全的。
证明问题是NP完全的过程被称为：*归约*（*reducing*或者是*reduction*）。
它的基本思想是：
可以在多项式时间内执行的问题之间创建转换。
因此，如果可以在多项式时间内解决我们的问题，就可以通过多项式变换，从而在多项式时间内求解所有已知的NP完全问题。
这也就是为什么说如果能够在多项式时间内解决任何一个NP完全问题，我们就可以在多项式时间内解决所有的这些问题的原因。

一个重要的需要你知道的知识是：
如果试图解决的问题是NP完全的，这也就意味着没有多项式时间的算法可以去解决它。
这可以防止你浪费时间去尝试找到更有效的算法。
当然，如果你的问题是NP完全的，而且你找到了一个多项式算法，那么你就解决了计算机科学里的一个开放问题。
知道你的问题是NP完全的还能够告诉你，如果你有一个很大的问题要解决，那么你不可能在合理的时间内解决它。
但是，你可以找到一个可以得到近似最优解的算法。
在某些情况下，你可能会证明你的近似算法产生的解是在最优解的某个百分比范围内。
比如说，我们可能可以找到一种算法，这个算法能够对旅行推销员问题得到一条路径，这个路径不会超过最优解的长度的两倍。

就像我们说过的，证明算法是NP完全的是一个更高级的主题，我们不会在这本书里详细地介绍它。
好在，其他数学家和计算机科学家已经证明了许多问题是NP完全问题。
有一本完整的书[^3]分类了一些NP完全问题，以及它们是如何被归约的。
如果你正在努力为你的问题提出有效的算法，那么第一步应该去检查已知的NP完全问题的列表，看看你的问题是不是一个已知的NP完全问题。

> [^3] Michael Garey和David Johnson著的《计算机和难解性NP完全性理论导引》（Computers and Intractability: A Guide to the Theory of NP-Completeness），纽约：弗里曼出版社, 1979

## 章节总结

这一章对我们在这本书里使用的一些算法策略进行了正式的分类，以及讨论了在什么时候可以使用哪一种策略。
了解这些技术以及在什么时候去应用它们可以帮助你开发算法来解决遇到的新问题。
下面总结了这一章里介绍的具体概念。

* 分治算法会把问题分解为子问题，然后把子问题的解组合起来解决初始问题。

* 分治算法通常使用递归来编写；主定理可以被用来分析许多递归算法的运行时间。

* 快速排序算法是最常用的排序算法。
在实践中，一个好的实现会让整个算法很快，但它在某些情况下它也可能很慢，这取决于要排序的元素的初始顺序和基准元素的选择。

* 贪婪算法通常被用来处理最优化问题，而且当选择当前的最优解可以得出初始问题的最优解的情况下，可以非常好的工作。
用来进行压缩的前置码算法是贪婪算法的一个很好的例子。

* 动态规划是一种类似于分治算法的策略。
它也经常被用来解决最优化问题。
当一个分治算法试图多次解决相同的子问题的时候，就应该使用动态规划来处理整个问题。
因此，我们不需要每次都去求解这个子问题，而是只解决这个问题一次，并且把答案存起来，从而能够在下一次这个子问题出现的时候直接使用。

* 存在许多问题都没有已知的多项式时间算法。
NP完全问题是这些问题的一个子集，而且它的一个有趣的特性是：
如果某个NP完全问题可以在多项式时间内得到解，那么所有的NP完全问题都可以在多项式时间内得到解。

## 练习

**判断题**

1. 快速排序永远都是最有效的排序算法。

2. 快速排序是一种分治算法。

3. 快速排序算法需要比归并排序算法更少的内存。

4. 基准元素的选择会影响快速排序算法的运行时间。

5. 所有的递归关系都可以用主定理来解决。

6. 贪心算法可以被用来解决所有的最优化问题。

7. 所有的分治问题都应该用动态规划来解决。

8. 动态规划算法会存储子问题的解，从而可以重用它们而不再需要去重新计算这些子问题。

9. 对于最长公共子序列问题，始终会有且只有一个答案。

10. 任何动态规划算法都可以使用记忆化来通过递归方式实现。

11. 如果我们可以在多项式时间内解决一个NP完全问题，我们就可以在多项式时间内解决所有的NP完全问题。

**选择题**

1. 使用主定理，递归关系$T(n) = 3T(n/2) + n$的解是什么？

    a) $Θ(n)$

    b) $Θ(n^{\log_2 3})$

    c) $Θ(n^{\log_2 3} ∗ \log_2n)$

    d) 它不能用主定理来解决。

2. 使用主定理，递归关系$T(n) = 4T(n/2) + n^2$的解是什么？

    a) $Θ(n)$

    b) $Θ(n^2)$

    c) $Θ(n^2 ∗ \log_2n)$

    d) 它不能用主定理来解决。

3. 使用主定理，递归关系$T(n) = 2T(n/3) + 2T(n/4) + n$的解是什么？

    b) $Θ(n^{\log_2 3})$

    b) $Θ(n^0.5)$

    c) $Θ(n^0.5 ∗ \log_2n)$

    d) 它不能用主定理来解决。

4. 什么情况下可以使用有损压缩，而不一定需要无损压缩？

    a) 在所有情况下

    b) 压缩程序的源代码

    c) 压缩可执行程序

    d) 当你不需要恢复到完整的原始版本的数据时

5. 在使用分治算法的时候，在什么情况下应该使用动态规划？

    a) 解决所有分治的问题的时候

    b) 当分治算法的运行时间不是$Θ(n * \log_2n)$的时候

    c) 当存在重复的子问题的时候

    d) 只有被用在最优化问题的时候

**简答题**

1. 如果列表里的所有元素都相同，我们的一开始那个（第一个）版本的快速排序实现会发生什么？
我们需要怎么修改我们的代码？

2. 给出三个硬币和一个总额，让贪婪策略不能够得出使用最少数量的硬币的解。

3. 对后面这个字母和它的频率的列表使用霍夫曼编码算法：`{a：2，b：3，c：6，d：12，e：24，f：9}`，展示你做的过程以及最终生成的前置码。

4. 五个矩阵的乘积的不同括号的总数是多少？

5. $n$个矩阵的矩阵链的动态规划算法的运行时间是多少（只确定最优的计算顺序，而不用去计算矩阵的乘积）？

**编程练习**

1. 查找一个能找到平面里最近的两个点的分治算法。
并且实现这个算法。

2. 通过使用霍夫曼编码，编写程序来压缩和解压缩文件。

3. 实现这一章章里描述的最长公共子序列算法。

4. 搜索更多关于Dan Hirschberg算法的细节，并用它来解决最长公共子序列问题。

5. 通过动态规划来实现矩阵链乘法问题。
